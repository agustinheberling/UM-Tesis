{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf3d4add",
   "metadata": {},
   "source": [
    "## Codigo del EDA que finalmente no se uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052ca33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. Tratamiento de valores nulos ===\n",
    "# Para num√©ricas: imputar con la mediana\n",
    "#for col in num_vars:\n",
    "#    if ech[col].isnull().any():\n",
    "#        ech[col] = ech[col].fillna(ech[col].median())\n",
    "\n",
    "# Para categ√≥ricas: imputar con una categor√≠a especial\n",
    "#for col in cat_vars:\n",
    "#    if ech[col].isnull().any():\n",
    "#        ech[col] = ech[col].fillna('No informado')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8501636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. Codificaci√≥n de variables categ√≥ricas ===\n",
    "# Convertir binarias codificadas como 1/2 o S√≠/No a 0/1\n",
    "#def codificar_binarias(df):\n",
    "#    for col in df.columns:\n",
    "#        if df[col].nunique() == 2:\n",
    "#            vals = sorted(df[col].dropna().unique())\n",
    "#            if set(vals) <= set([0,1,2]):\n",
    "#                df[col] = df[col].map({vals[0]: 0, vals[1]: 1})\n",
    "#    return df\n",
    "\n",
    "#ech = codificar_binarias(ech)\n",
    "\n",
    "# One-hot encoding para variables categ√≥ricas no binarias\n",
    "#ech_encoded = pd.get_dummies(ech, columns=cat_vars, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7ac67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== PASO 2: Preprocesamiento (ORIGINAL)====== \n",
    "\n",
    "# Codificaci√≥n de ordinales ‚Äì si los valores est√°n bien ordenados\n",
    "#ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "# Escalador para continuas\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# Codificador para nominales\n",
    "#onehot = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# ColumnTransformer para aplicar todo junto\n",
    "#preprocessor = ColumnTransformer(transformers=[\n",
    "#    ('nom', onehot, nominales),\n",
    "#    ('ord', ordinal_encoder, ordinales),\n",
    "#    ('scale', scaler, numericas_continuas)\n",
    "#], remainder='passthrough')  # deja discretas y dicot√≥micas sin tocar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a6c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== PASO 3: Separar X e y ======\n",
    "\n",
    "#X = ech[dicotomicas + nominales + ordinales + numericas_discretas + numericas_continuas]\n",
    "\n",
    "# Eliminar variables con fuga de informaci√≥n\n",
    "#vars_a_eliminar = ['YDA', 'YDA_SVL']  # ajustar seg√∫n auditor√≠a\n",
    "#X = X.drop(columns=[col for col in vars_a_eliminar if col in X.columns])\n",
    "\n",
    "#y = ech['log_YDA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709de845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== PASO 5: Pipeline final ======\n",
    "\n",
    "#pipeline = Pipeline(steps=[\n",
    "#    ('preprocesamiento', preprocessor)\n",
    "#])\n",
    "\n",
    "# Fit-transform solo para ver el shape de salida\n",
    "#X_train_proc = pipeline.fit_transform(X_train)\n",
    "#X_test_proc = pipeline.transform(X_test)\n",
    "#print(\"Shape del dataset procesado:\", X_train_proc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ada2ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Filtrar listas de variables para que no incluyan columnas eliminadas\n",
    "#dicotomicas = [c for c in dicotomicas if c not in vars_a_eliminar]\n",
    "#nominales = [c for c in nominales if c not in vars_a_eliminar]\n",
    "#ordinales = [c for c in ordinales if c not in vars_a_eliminar]\n",
    "#numericas_discretas = [c for c in numericas_discretas if c not in vars_a_eliminar]\n",
    "#numericas_continuas = [c for c in numericas_continuas if c not in vars_a_eliminar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5463edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Crear carpeta \"data_processed\" dentro del proyecto si no existe\n",
    "output_dir = \"data_processed\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Guardar X e y en CSV\n",
    "X.to_csv(os.path.join(output_dir, \"X_clean.csv\"), index=False)\n",
    "y.to_csv(os.path.join(output_dir, \"y_clean.csv\"), index=False)\n",
    "\n",
    "print(\"‚úÖ Archivos guardados en carpeta 'data_processed':\")\n",
    "print(f\"- {os.path.join(output_dir, 'X_clean.csv')}\")\n",
    "print(f\"- {os.path.join(output_dir, 'y_clean.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc43cceb",
   "metadata": {},
   "source": [
    "Funciones de limpieza que haciamos luego del train_test_split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec1a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2Ô∏è‚É£ Funciones de limpieza\n",
    "# ============================================\n",
    "def eliminar_varianza_cero(df):\n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "    selector = VarianceThreshold(threshold=0.0)\n",
    "    selector.fit(df)\n",
    "    cols_quitar = df.columns[~selector.get_support()].tolist()\n",
    "    df_filtrado = df.drop(columns=cols_quitar)\n",
    "    return df_filtrado, cols_quitar\n",
    "\n",
    "def eliminar_correlacion_perfecta(df):\n",
    "    corr_matrix = df.corr()\n",
    "    cols_quitar = set()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if corr_matrix.iloc[i, j] == 1:\n",
    "                colname = corr_matrix.columns[i]\n",
    "                cols_quitar.add(colname)\n",
    "    df_filtrado = df.drop(columns=list(cols_quitar))\n",
    "    return df_filtrado, list(cols_quitar)\n",
    "\n",
    "def eliminar_correlacion_alta(df, umbral=0.95):\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    cols_quitar = [column for column in upper.columns if any(upper[column] > umbral)]\n",
    "    df_filtrado = df.drop(columns=cols_quitar)\n",
    "    return df_filtrado, cols_quitar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3Ô∏è‚É£ Aplicar limpieza a TRAIN y TEST\n",
    "# ============================================\n",
    "# Varianza cero\n",
    "X_train_proc, cols_var0 = eliminar_varianza_cero(X_train_proc)\n",
    "X_test_proc = X_test_proc.drop(columns=cols_var0, errors='ignore')\n",
    "\n",
    "# Correlaci√≥n perfecta\n",
    "X_train_proc, cols_corr1 = eliminar_correlacion_perfecta(X_train_proc)\n",
    "X_test_proc = X_test_proc.drop(columns=cols_corr1, errors='ignore')\n",
    "\n",
    "# Correlaci√≥n alta\n",
    "X_train_proc, cols_corr_high = eliminar_correlacion_alta(X_train_proc, umbral=0.95)\n",
    "X_test_proc = X_test_proc.drop(columns=cols_corr_high, errors='ignore')\n",
    "\n",
    "print(f\"üìâ Shape final tras limpieza: {X_train_proc.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806bfe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4Ô∏è‚É£ Guardar columnas eliminadas para trazabilidad\n",
    "# ============================================\n",
    "df_eliminadas = pd.DataFrame({\n",
    "    \"varianza_cero\": pd.Series(cols_var0),\n",
    "    \"corr_perfecta\": pd.Series(cols_corr1),\n",
    "    \"corr_alta\": pd.Series(cols_corr_high)\n",
    "})\n",
    "df_eliminadas.to_excel(\"columnas_eliminadas.xlsx\", index=False)\n",
    "print(\"üìÇ Archivo 'columnas_eliminadas.xlsx' guardado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64493f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar datasets limpios\n",
    "X = pd.read_csv(\"data_processed/X_clean.csv\")\n",
    "y = pd.read_csv(\"data_processed/y_clean.csv\").squeeze()  # .squeeze() para que sea Serie y no DataFrame\n",
    "\n",
    "print(\"Shapes cargados:\")\n",
    "print(\"X:\", X.shape)\n",
    "print(\"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8270ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importo librerias\n",
    "#from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "#from sklearn.tree import DecisionTreeRegressor\n",
    "#from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.metrics import make_scorer, mean_squared_error\n",
    "#import numpy as np\n",
    "#import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c8902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar XGBoost y LightGBM si est√°n disponibles\n",
    "#try:\n",
    "#    from xgboost import XGBRegressor\n",
    "#except ImportError:\n",
    "#    XGBRegressor = None\n",
    "\n",
    "#try:\n",
    "#    from lightgbm import LGBMRegressor\n",
    "#except ImportError:\n",
    "#    LGBMRegressor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56260ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelos a evaluar\n",
    "#modelos = {\n",
    "#    \"LinearRegression\": LinearRegression(),\n",
    "#    \"Ridge\": Ridge(),\n",
    "#    \"Lasso\": Lasso(),\n",
    "#    \"DecisionTree\": DecisionTreeRegressor(),\n",
    "#    \"RandomForest\": RandomForestRegressor(n_jobs=-1),\n",
    "#    \"GradientBoosting\": GradientBoostingRegressor()\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfd4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar XGBoost si est√° disponible\n",
    "#if XGBRegressor:\n",
    "#    modelos[\"XGBoost\"] = XGBRegressor(n_jobs=-1)\n",
    "\n",
    "# Agregar LightGBM si est√° disponible\n",
    "#if LGBMRegressor:\n",
    "#    modelos[\"LightGBM\"] = LGBMRegressor(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ee5063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©trica: MSE negativo (lo convierte a positivo m√°s abajo)\n",
    "#scoring = make_scorer(mean_squared_error, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a5deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecuta validaci√≥n cruzada y muestra resultados\n",
    "#resultados = {}\n",
    "#for nombre, modelo in modelos.items():\n",
    "#    scores = cross_val_score(modelo, X_train_proc, y_train, cv=5, scoring=scoring)\n",
    "#    mse_promedio = -np.mean(scores)\n",
    "#    print(f\"{nombre}: MSE promedio = {mse_promedio:,.2f}\")\n",
    "#    resultados[nombre] = mse_promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1d950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar resultados ordenados\n",
    "#df_resultados = pd.DataFrame(resultados).T.sort_values(by='RMSE promedio')\n",
    "#print(\"\\nComparaci√≥n de modelos por RMSE (menor es mejor):\")\n",
    "#display(df_resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2cfec5",
   "metadata": {},
   "source": [
    "Primera funcion para evaluar modelo (la comentamos para no correrla)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf64753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilidad para evaluar en ambas escalas\n",
    "#def evaluar_modelo(modelo, Xtr, ytr, Xte, yte, nombre=\"Modelo\"):\n",
    "    # Entrenar\n",
    "#    modelo.fit(Xtr, ytr)\n",
    "    # Predecir\n",
    "#    y_pred = modelo.predict(Xte)\n",
    "\n",
    "    # M√©tricas en escala log (si y es log_YDA)\n",
    "#    mae = mean_absolute_error(yte, y_pred)\n",
    "#    mse = mean_squared_error(yte, y_pred)\n",
    "#    rmse = np.sqrt(mse)\n",
    "#    r2 = r2_score(yte, y_pred)\n",
    "\n",
    "#    print(f\"\\n=== {nombre} | M√©tricas en escala log (log_YDA) ===\")\n",
    "#    print(f\"MAE : {mae:,.4f}\")\n",
    "#    print(f\"MSE : {mse:,.4f}\")\n",
    "#    print(f\"RMSE: {rmse:,.4f}\")\n",
    "#    print(f\"R¬≤  : {r2:,.4f}\")\n",
    "\n",
    "    # M√©tricas en escala original (deslogarizando)\n",
    "#    yte_orig = np.expm1(yte)\n",
    "#    y_pred_orig = np.expm1(y_pred)\n",
    "\n",
    "#    mae_o = mean_absolute_error(yte_orig, y_pred_orig)\n",
    "#    mse_o = mean_squared_error(yte_orig, y_pred_orig)\n",
    "#    rmse_o = np.sqrt(mse_o)\n",
    "#    r2_o = r2_score(yte_orig, y_pred_orig)\n",
    "\n",
    "#    print(f\"\\n=== {nombre} | M√©tricas en escala original (pesos) ===\")\n",
    "#    print(f\"MAE : {mae_o:,.2f}\")\n",
    "#    print(f\"MSE : {mse_o:,.2f}\")\n",
    "#    print(f\"RMSE: {rmse_o:,.2f}\")\n",
    "#    print(f\"R¬≤  : {r2_o:,.4f}\")\n",
    "\n",
    "#    return {\n",
    "#        \"mae_log\": mae, \"mse_log\": mse, \"rmse_log\": rmse, \"r2_log\": r2,\n",
    "#        \"mae\": mae_o, \"mse\": mse_o, \"rmse\": rmse_o, \"r2\": r2_o,\n",
    "#        \"y_pred\": y_pred, \"y_pred_orig\": y_pred_orig\n",
    "#    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08806158",
   "metadata": {},
   "source": [
    "Funcion para evaluar modelos con metricas solo en test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5cdee3",
   "metadata": {},
   "outputs": [],
   "source": [
    " #print(\"üìä Calculando m√©tricas en escala log...\")\n",
    "    #r2 = r2_score(y_test, y_pred_test)\n",
    "    #mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    #rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "    # üîÅ Revertimos a escala de pesos, evitando overflow en np.exp\n",
    "    #y_test_clip = np.clip(y_test, 0, 30)\n",
    "    #y_pred_clip = np.clip(y_pred_test, 0, 30)\n",
    "\n",
    "    #y_test_original = np.exp(y_test_clip)\n",
    "    #y_pred_original = np.exp(y_pred_clip)\n",
    "\n",
    "    #print(f\"M√°ximo y_pred_original (clipped): {np.max(y_pred_original)}\")\n",
    "\n",
    "    #print(\"üìä Calculando m√©tricas en pesos...\")\n",
    "    #mae_pesos = mean_absolute_error(y_test_original, y_pred_original)\n",
    "    #rmse_pesos = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n",
    "\n",
    "    #print(\"‚úÖ M√©tricas finales:\")\n",
    "    #print(f\"Modelo: {nombre}\")\n",
    "    #print(f\"R2: {r2:.4f} | MAE_log: {mae:.2f} | RMSE_log: {rmse:.2f}\")\n",
    "    #print(f\"MAE_pesos: {mae_pesos:.2f} | RMSE_pesos: {rmse_pesos:.2f}\")\n",
    "\n",
    "    #return {\n",
    "    #    \"modelo\": nombre,\n",
    "    #    \"R2\": r2,\n",
    "    #    \"MAE_log\": mae,\n",
    "    #    \"RMSE_log\": rmse,\n",
    "    #    \"MAE_pesos\": mae_pesos,\n",
    "    #    \"RMSE_pesos\": rmse_pesos\n",
    "    #}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98623ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Split en crudo (sin preprocesar a√∫n)\n",
    "\n",
    "# X = ech[dicotomicas + nominales + ordinales + numericas_discretas + numericas_continuas]\n",
    "# y = ech['log_YDA']   # ya definido previamente\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1052ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Ajustar SOLO con TRAIN y transformar TRAIN y TEST:\n",
    "# El preprocessor es el ColumnTransformer definido antes.\n",
    "\n",
    "#preprocessor.fit(X_train)\n",
    "\n",
    "#X_train_proc = preprocessor.transform(X_train)\n",
    "#X_test_proc  = preprocessor.transform(X_test)\n",
    "\n",
    "#print(\"Shapes tras preprocesamiento:\")\n",
    "#print(\"X_train_proc:\", X_train_proc.shape)\n",
    "#print(\"X_test_proc :\", X_test_proc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd42e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_lr = LinearRegression()\n",
    "\n",
    "try:\n",
    "    modelo_lr.fit(X_train_proc, y_train)\n",
    "    print(\"‚úÖ Fit correcto\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error durante el fit:\", e)\n",
    "\n",
    "try:\n",
    "    y_pred = modelo_lr.predict(X_test_proc)\n",
    "    print(\"‚úÖ Predict correcto\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error durante el predict:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418c4dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Fit-transform SOLO con train y transform test\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_test_proc  = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Shapes tras preprocesamiento:\")\n",
    "print(\"X_train_proc:\", X_train_proc.shape)\n",
    "print(\"X_test_proc :\", X_test_proc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab14ca5",
   "metadata": {},
   "source": [
    "Hacemos un VIF (Variance Inflation Factor) de las variables procesadas para detectar multicolinealidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579ec684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 1) Calcular VIF en datos procesados\n",
    "# =========================================\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# X_train_proc es un array numpy, as√≠ que lo pasamos a DataFrame\n",
    "# Usamos las columnas generadas por el preprocessor\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "X_proc_df = pd.DataFrame(X_proc, columns=feature_names)\n",
    "\n",
    "# Calculamos VIF\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_proc_df.columns\n",
    "vif_data[\"VIF\"] = [\n",
    "    variance_inflation_factor(X_proc_df.values, i) for i in range(X_proc_df.shape[1])\n",
    "]\n",
    "\n",
    "# Ordenamos por VIF descendente\n",
    "vif_data = vif_data.sort_values(by=\"VIF\", ascending=False)\n",
    "print(\"\\nüìä Top 20 features con mayor VIF:\")\n",
    "print(vif_data.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677f3c64",
   "metadata": {},
   "source": [
    "üìä Top 20 features con mayor VIF:\n",
    "              feature  VIF\n",
    "1442  nom__f72_2_8413  inf\n",
    "1409  nom__f72_2_7210  inf\n",
    "1423  nom__f72_2_7830  inf\n",
    "1422  nom__f72_2_7820  inf\n",
    "1421  nom__f72_2_7810  inf\n",
    "1420  nom__f72_2_7730  inf\n",
    "1419  nom__f72_2_7729  inf\n",
    "1418  nom__f72_2_7721  inf\n",
    "1417  nom__f72_2_7710  inf\n",
    "1416  nom__f72_2_7500  inf\n",
    "1415  nom__f72_2_7490  inf\n",
    "1414  nom__f72_2_7420  inf\n",
    "1413  nom__f72_2_7410  inf\n",
    "1412  nom__f72_2_7320  inf\n",
    "1411  nom__f72_2_7310  inf\n",
    "1410  nom__f72_2_7220  inf\n",
    "1408  nom__f72_2_7120  inf\n",
    "1590      nom__f305_2  inf\n",
    "1407  nom__f72_2_7110  inf\n",
    "1406  nom__f72_2_7020  inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f4d121",
   "metadata": {},
   "source": [
    "Esos resultados son una se√±al clar√≠sima de que tienes multicolinealidad perfecta en muchas variables, y adem√°s concentrada en un bloque muy espec√≠fico: las columnas nom__f72_2_XXXX y alguna como nom__f305_2.\n",
    "\n",
    "Que el VIF sea inf significa que alguna de esas variables es combinaci√≥n lineal exacta de otras, lo cual es t√≠pico cuando:\n",
    "\n",
    "Hay One-Hot Encoding con categor√≠as que nunca se presentan en train o tienen representaci√≥n redundante.\n",
    "\n",
    "Se incluyen todas las dummies de una variable categ√≥rica (no se hace drop='first').\n",
    "\n",
    "Hay columnas que son copias exactas o que sumadas a otras siempre dan lo mismo (dummy trap)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5cbd94",
   "metadata": {},
   "source": [
    "üí° Conclusiones pr√°cticas:\n",
    "\n",
    "Tu OneHotEncoder probablemente est√° generando columnas completamente redundantes en algunas categor√≠as (las de f72_2 parecen ser un caso de codificaci√≥n con much√≠simas categor√≠as).\n",
    "\n",
    "Esto explica por qu√© Linear Regression explotaba** ‚Üí la matriz X'X es singular o casi singular.\n",
    "\n",
    "Ridge puede estabilizar algo, pero si el VIF es inf es porque tienes redundancia total; lo ideal ser√≠a reducir esas columnas.\n",
    "\n",
    "Dado que ya usas drop='first' en el OneHotEncoder, podr√≠a estar pasando que algunas categor√≠as est√°n 100% correlacionadas con otras variables (por ejemplo, dummies para ubicaciones que coinciden con otra variable geogr√°fica)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e898cf1",
   "metadata": {},
   "source": [
    "Modelo de Ridge Regression original que no usaba la funcion evaluar_modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b2bf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 2) Entrenar Ridge Regression\n",
    "# =========================================\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "modelo_ridge = Ridge(alpha=10)  # alpha controla regularizaci√≥n; probar 1, 10, 100\n",
    "modelo_ridge.fit(X_train, y_train)\n",
    "\n",
    "y_pred = modelo_ridge.predict(X_test_proc)\n",
    "\n",
    "# Evaluaci√≥n\n",
    "mae_log = mean_absolute_error(y_test, y_pred)\n",
    "mse_log = mean_squared_error(y_test, y_pred)\n",
    "rmse_log = np.sqrt(mse_log)\n",
    "r2_log = r2_score(y_test, y_pred)\n",
    "\n",
    "# Invertimos escala log para m√©tricas en pesos\n",
    "y_pred_original = np.exp(y_pred)\n",
    "y_test_original = np.exp(y_test)\n",
    "\n",
    "mae_pesos = mean_absolute_error(y_test_original, y_pred_original)\n",
    "rmse_pesos = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n",
    "\n",
    "print(\"\\n‚úÖ M√©tricas Ridge Regression:\")\n",
    "print(f\"R2: {r2_log:.4f} | MAE_log: {mae_log:.4f} | RMSE_log: {rmse_log:.4f}\")\n",
    "print(f\"MAE_pesos: {mae_pesos:.2f} | RMSE_pesos: {rmse_pesos:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ac56b7",
   "metadata": {},
   "source": [
    "Auditor√≠a de features (Feature Importance)\n",
    "\n",
    "Esto te permite ver cu√°les son las variables m√°s influyentes seg√∫n el modelo. Si aparece una variable que contiene informaci√≥n directa del ingreso (por ejemplo, otro tipo de ingreso o una suma de ingresos del hogar), entonces es probable que haya fuga de informaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5dd52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Obtener nombres de las columnas procesadas (despu√©s del pipeline)\n",
    "# Esto asume que hiciste fit en el pipeline con X_train\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "importancias = modelo_rf.feature_importances_\n",
    "\n",
    "# Crear DataFrame con los resultados\n",
    "importancias_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importancias\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Mostrar las 20 m√°s importantes\n",
    "print(importancias_df.head(20))\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(importancias_df.head(20)['feature'][::-1], importancias_df.head(20)['importance'][::-1])\n",
    "plt.xlabel(\"Importancia\")\n",
    "plt.title(\"Top 20 variables m√°s importantes - Random Forest\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa76b943",
   "metadata": {},
   "source": [
    "Observamos:\n",
    "- La variable num__YDA aporta el 99.8% de la importancia del modelo.\n",
    "- El resto de las variables tienen importancias casi nulas (valores del orden de 0.001 o incluso 0.000002).\n",
    "- Entre las m√°s importantes figuran tambi√©n HT11, YDA_SVL y YSVL, todas ellas potencialmente relacionadas con ingreso\n",
    "\n",
    "Interpretaci√≥n:\n",
    "- Esto es una se√±al clara de fuga de informaci√≥n.\n",
    "- YDA es tu variable objetivo (aunque aqu√≠ en su versi√≥n logar√≠tmica), y si aparece como predictor, entonces el modelo est√° simplemente \"viendo\" la respuesta, lo cual hace in√∫tiles las m√©tricas obtenidas.\n",
    "- Variables como YDA_SVL o YSVL tambi√©n suenan sospechosamente similares, por lo que es probable que sean funciones directas o indirectas del ingreso.\n",
    "\n",
    "Conclusi√≥n de esta parte:\n",
    "- Tu modelo est√° dominado por variables que contienen directamente (o de forma colineal) la variable que intent√°s predecir. \n",
    "- Esto invalida los resultados. Se debe excluir YDA y toda variable derivada del conjunto de predictores.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4194e753",
   "metadata": {},
   "source": [
    "Random Forest con Validaci√≥n cruzada (Cross-Validation)\n",
    "\n",
    "Esto te dice si el rendimiento es estable entre distintas particiones de los datos. Si hay sobreajuste, el resultado promedio ser√° mucho m√°s bajo que el obtenido en tu partici√≥n test anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87ab46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Pipeline que combina preprocesamiento y modelo\n",
    "modelo_rf_cv = Pipeline(steps=[\n",
    "    ('preprocesamiento', preprocessor),\n",
    "    ('modelo', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Ejecutar CV con el pipeline\n",
    "scores = cross_val_score(modelo_rf_cv, X, y, cv=5, scoring='r2')\n",
    "\n",
    "print(\"R¬≤ por fold:\", scores)\n",
    "print(f\"Media R¬≤: {scores.mean():.4f}\")\n",
    "print(f\"Desviaci√≥n est√°ndar: {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1d84ed",
   "metadata": {},
   "source": [
    "Observamos:\n",
    "\n",
    "Nivel de rendimiento:\n",
    "- Media R¬≤ ‚âà 0.8576 significa que, en promedio, el modelo explica alrededor del 85,8% de la variabilidad del ingreso en los datos de validaci√≥n.\n",
    "- Esto es menor al R¬≤ ‚âà 0.94 que obtuviste en el train/test split √∫nico, lo que es normal porque la validaci√≥n cruzada da una estimaci√≥n m√°s realista y menos optimista.\n",
    "\n",
    "Estabilidad del modelo:\n",
    "- Desviaci√≥n est√°ndar: 0.0127 ‚Üí muy baja, lo que indica que el rendimiento es consistente entre folds y que el modelo es estable ante cambios en el conjunto de entrenamiento.\n",
    "- Un desv√≠o bajo sugiere que el modelo no depende demasiado de un subconjunto particular de los datos.\n",
    "\n",
    "Posible sobreajuste:\n",
    "- La diferencia entre el R¬≤ alto en train/test √∫nico y el R¬≤ de CV sugiere que el modelo podr√≠a estar sobreajustando levemente.\n",
    "- Esto se puede trabajar ajustando hiperpar√°metros (max_depth, min_samples_split, etc.) o reduciendo dimensionalidad.\n",
    "\n",
    "Conclusi√≥n:\n",
    "- El modelo es fuerte y estable, pero el gap entre test √∫nico y CV sugiere que todav√≠a hay margen para mejorar generalizaci√≥n.\n",
    "- Ya est√° en un rango de rendimiento alto para este tipo de problema, as√≠ que es un buen candidato para afinar con GridSearch o RandomizedSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a4640b",
   "metadata": {},
   "source": [
    "Random Forest con GridSearchCV explota la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631f623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Random Forest con GridSearchCV (Modelo 5)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# B√∫squeda en una grilla compacta pero informativa\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [300, 500, 800],\n",
    "    \"max_depth\": [None, 12, 20],\n",
    "    \"min_samples_split\": [2, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "rf_base = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Usamos RMSE negativo como scoring (minimizar RMSE)\n",
    "grid_rf = GridSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_grid=param_grid_rf,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Ajuste con TRAIN ya preprocesado\n",
    "grid_rf.fit(X_train_proc, y_train)\n",
    "\n",
    "print(\"\\n=== GridSearchCV RandomForest ===\")\n",
    "print(\"Mejores hiperpar√°metros:\", grid_rf.best_params_)\n",
    "print(f\"Mejor score CV (RMSE): {-grid_rf.best_score_:,.4f}\")\n",
    "\n",
    "# Evaluaci√≥n final en TEST con el mejor estimador\n",
    "mejor_rf = grid_rf.best_estimator_\n",
    "res_rf_cv = evaluar_modelo(mejor_rf, X_train_proc, y_train, X_test_proc, y_test, nombre=\"Random Forest (GridSearchCV)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6566a7bc",
   "metadata": {},
   "source": [
    "Recordamos:\n",
    "- Los √°rboles/boosting suelen rendir mejor que los lineales en este tipo de datos tabulares con alta no linealidad e interacciones.\n",
    "- Si el tiempo es un factor, pod√©s reducir la grilla del RF (menos combinaciones) o usar RandomizedSearchCV.\n",
    "- Si tu y es log_YDA, las m√©tricas ‚Äúoriginales‚Äù que imprime el bloque (deslogarizando) son las m√°s interpretables para presentaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31091e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV XGBoost\n",
    "\n",
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "# Scoring de RMSE (negativo para que lo maximice)\n",
    "scoring_rmse = \"neg_root_mean_squared_error\"\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    from scipy.stats import randint, uniform, loguniform\n",
    "\n",
    "    xgb_base = XGBRegressor(\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\"   # r√°pido y estable\n",
    "    )\n",
    "\n",
    "    dist_xgb = {\n",
    "        \"n_estimators\": randint(400, 1500),\n",
    "        \"max_depth\": randint(3, 12),\n",
    "        \"learning_rate\": loguniform(1e-3, 3e-1),\n",
    "        \"subsample\": uniform(0.6, 0.4),         # [0.6, 1.0]\n",
    "        \"colsample_bytree\": uniform(0.6, 0.4),  # [0.6, 1.0]\n",
    "        \"reg_lambda\": loguniform(1e-3, 10),\n",
    "        \"reg_alpha\": loguniform(1e-4, 1)\n",
    "    }\n",
    "\n",
    "    rnd_xgb = RandomizedSearchCV(\n",
    "        estimator=xgb_base,\n",
    "        param_distributions=dist_xgb,\n",
    "        n_iter=40,              # pod√©s subir/bajar seg√∫n tiempo\n",
    "        scoring=scoring_rmse,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    rnd_xgb.fit(X_train_proc, y_train)\n",
    "    print(\"\\n=== RandomizedSearchCV XGBoost ===\")\n",
    "    print(\"Mejores hiperpar√°metros:\", rnd_xgb.best_params_)\n",
    "    print(f\"Mejor score CV (RMSE): {-rnd_xgb.best_score_:,.4f}\")\n",
    "\n",
    "    best_xgb = rnd_xgb.best_estimator_\n",
    "    res_xgb = evaluar_modelo(best_xgb, X_train_proc, y_train, X_test_proc, y_test, nombre=\"XGBoost (RandomizedSearchCV)\")\n",
    "\n",
    "except ImportError:\n",
    "    best_xgb = None\n",
    "    print(\"\\n[AVISO] xgboost no est√° instalado. Ejecuta: pip install xgboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40030a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV LightGBM\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    from scipy.stats import randint, uniform, loguniform\n",
    "\n",
    "    lgb_base = LGBMRegressor(\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    dist_lgb = {\n",
    "        \"n_estimators\": randint(400, 1500),\n",
    "        \"learning_rate\": loguniform(1e-3, 3e-1),\n",
    "        \"num_leaves\": randint(31, 255),\n",
    "        \"max_depth\": [-1, 6, 8, 10, 12],          # categ√≥rico\n",
    "        \"subsample\": uniform(0.6, 0.4),\n",
    "        \"colsample_bytree\": uniform(0.6, 0.4),\n",
    "        \"reg_lambda\": loguniform(1e-3, 10),\n",
    "        \"min_child_samples\": randint(5, 100)\n",
    "    }\n",
    "\n",
    "    rnd_lgb = RandomizedSearchCV(\n",
    "        estimator=lgb_base,\n",
    "        param_distributions=dist_lgb,\n",
    "        n_iter=40,\n",
    "        scoring=scoring_rmse,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    rnd_lgb.fit(X_train_proc, y_train)\n",
    "    print(\"\\n=== RandomizedSearchCV LightGBM ===\")\n",
    "    print(\"Mejores hiperpar√°metros:\", rnd_lgb.best_params_)\n",
    "    print(f\"Mejor score CV (RMSE): {-rnd_lgb.best_score_:,.4f}\")\n",
    "\n",
    "    best_lgb = rnd_lgb.best_estimator_\n",
    "    res_lgb = evaluar_modelo(best_lgb, X_train_proc, y_train, X_test_proc, y_test, nombre=\"LightGBM (RandomizedSearchCV)\")\n",
    "\n",
    "except ImportError:\n",
    "    best_lgb = None\n",
    "    print(\"\\n[AVISO] lightgbm no est√° instalado. Ejecuta: pip install lightgbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac51a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation Importance (sobre el mejor modelo)\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Obtener nombres de features del preprocessor ya fiteado\n",
    "try:\n",
    "    feature_names = preprocessor.get_feature_names_out(X.columns)\n",
    "except Exception:\n",
    "    # fallback: √≠ndices si no hay nombres\n",
    "    feature_names = np.array([f\"f{i}\" for i in range(X_train_proc.shape[1])])\n",
    "\n",
    "# 2) Elegir el mejor modelo entre XGB y LGB (por RMSE en test)\n",
    "candidatos = []\n",
    "if 'res_xgb' in locals():\n",
    "    candidatos.append((\"XGBoost\", best_xgb, res_xgb['rmse']))\n",
    "if 'res_lgb' in locals():\n",
    "    candidatos.append((\"LightGBM\", best_lgb, res_lgb['rmse']))\n",
    "\n",
    "if candidatos:\n",
    "    ganador_nombre, ganador_modelo, _ = sorted(candidatos, key=lambda t: t[2])[0]\n",
    "    print(f\"\\nModelo ganador para interpretaci√≥n: {ganador_nombre}\")\n",
    "else:\n",
    "    # Si no hay XGB/LGB disponibles, pod√©s elegir otro (ej: mejor RF)\n",
    "    ganador_nombre, ganador_modelo = \"Modelo_no_disponible\", None\n",
    "\n",
    "# 3) Permutation importance (si hay modelo)\n",
    "if ganador_modelo is not None:\n",
    "    # Para acelerar, usar una muestra del test (opcional)\n",
    "    np.random.seed(42)\n",
    "    idx_sample = np.random.choice(X_test_proc.shape[0], size=min(3000, X_test_proc.shape[0]), replace=False)\n",
    "    X_te_pi = X_test_proc[idx_sample]\n",
    "    y_te_pi = y_test.iloc[idx_sample] if hasattr(y_test, \"iloc\") else y_test[idx_sample]\n",
    "\n",
    "    print(\"\\nCalculando Permutation Importance (esto puede tardar un poco)...\")\n",
    "    pi = permutation_importance(\n",
    "        estimator=ganador_modelo,\n",
    "        X=X_te_pi,\n",
    "        y=y_te_pi,\n",
    "        n_repeats=5,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    importancias = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance_mean\": pi.importances_mean,\n",
    "        \"importance_std\": pi.importances_std\n",
    "    }).sort_values(\"importance_mean\", ascending=False)\n",
    "\n",
    "    top = importancias.head(20)\n",
    "    print(\"\\nTop 20 features por Permutation Importance:\")\n",
    "    display(top)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    plt.barh(top[\"feature\"][::-1], top[\"importance_mean\"][::-1])\n",
    "    plt.title(f\"Permutation Importance - Top 20 ({ganador_nombre})\")\n",
    "    plt.xlabel(\"Mean decrease in score (|neg RMSE|)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n[AVISO] No hay modelo ganador para calcular permutation importance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f31a311",
   "metadata": {},
   "source": [
    "# SHAP para el modelo ganador\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "\n",
    "    if ganador_modelo is not None:\n",
    "        # Crear DataFrame con nombres de columnas (mejora los gr√°ficos)\n",
    "        X_te_df = pd.DataFrame(X_test_proc, columns=feature_names)\n",
    "\n",
    "        # Muestra para acelerar el c√≥mputo\n",
    "        np.random.seed(42)\n",
    "        samp = min(2000, X_te_df.shape[0])\n",
    "        X_te_sample = X_te_df.sample(samp, random_state=42)\n",
    "\n",
    "        # Explainer para modelos de √°rboles (XGB/LGB/RF)\n",
    "        explainer = shap.TreeExplainer(ganador_modelo)\n",
    "        shap_values = explainer.shap_values(X_te_sample)\n",
    "\n",
    "        # Resumen global\n",
    "        shap.summary_plot(shap_values, X_te_sample, show=True, max_display=25)\n",
    "        # (Opcional) resumen por bar:\n",
    "        shap.summary_plot(shap_values, X_te_sample, plot_type=\"bar\", show=True, max_display=25)\n",
    "\n",
    "        # Nota: los SHAP est√°n en la misma escala que el target (log_YDA).\n",
    "    else:\n",
    "        print(\"\\n[AVISO] No hay modelo ganador para calcular SHAP.\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\n[AVISO] shap no est√° instalado. Ejecuta: pip install shap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8814db7",
   "metadata": {},
   "source": [
    "Consideramos:\n",
    "- Nombres de features: preprocessor.get_feature_names_out(X.columns) te devuelve los nombres ya expandidos por el OneHot; as√≠ los gr√°ficos y tablas quedan legibles.\n",
    "- Tiempo de c√≥mputo: tanto RandomizedSearchCV como SHAP pueden tardar; por eso limito n_iter y tomo muestras de test para importancia/SHAP.\n",
    "- Interpretaci√≥n: SHAP y permutation importance reportan contribuciones en escala de y. Como vos us√°s log_YDA, las contribuciones est√°n en log-ingreso (lo se√±al√°s en el informe)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47c91ab",
   "metadata": {},
   "source": [
    "Tabla comparativa de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76416964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Funci√≥n auxiliar para evaluar en log y en escala original (pesos)\n",
    "def evaluar_modelo_full(nombre, modelo, X_tr_proc, X_te_proc, y_tr, y_te):\n",
    "    # En log\n",
    "    y_pred_train = modelo.predict(X_tr_proc)\n",
    "    y_pred_test  = modelo.predict(X_te_proc)\n",
    "\n",
    "    mae_log  = mean_absolute_error(y_te, y_pred_test)\n",
    "    rmse_log = np.sqrt(mean_squared_error(y_te, y_pred_test))\n",
    "    r2_log   = r2_score(y_te, y_pred_test)\n",
    "\n",
    "    # A escala original\n",
    "    y_te_pesos    = np.exp(y_te)\n",
    "    y_pred_pesos  = np.exp(y_pred_test)\n",
    "\n",
    "    mae_pesos  = mean_absolute_error(y_te_pesos, y_pred_pesos)\n",
    "    rmse_pesos = np.sqrt(mean_squared_error(y_te_pesos, y_pred_pesos))\n",
    "    r2_pesos   = r2_score(y_te_pesos, y_pred_pesos)\n",
    "\n",
    "    return {\n",
    "        \"Modelo\": nombre,\n",
    "        \"MAE_log\": mae_log,\n",
    "        \"RMSE_log\": rmse_log,\n",
    "        \"R2_log\": r2_log,\n",
    "        \"MAE_pesos\": mae_pesos,\n",
    "        \"RMSE_pesos\": rmse_pesos,\n",
    "        \"R2_pesos\": r2_pesos\n",
    "    }\n",
    "\n",
    "# Lista de modelos ya entrenados y sus nombres\n",
    "modelos_resultados = []\n",
    "\n",
    "# Modelo 1: Regresi√≥n lineal\n",
    "modelos_resultados.append(evaluar_modelo_full(\"LinearRegression\", modelo_lr, X_train_proc, X_test_proc, y_train, y_test))\n",
    "\n",
    "# Modelo 2: Random Forest b√°sico\n",
    "modelos_resultados.append(evaluar_modelo_full(\"RandomForest\", modelo_rf, X_train_proc, X_test_proc, y_train, y_test))\n",
    "\n",
    "# Modelo 3: XGBoost optimizado\n",
    "modelos_resultados.append(evaluar_modelo_full(\"XGBoost\", best_xgb, X_train_proc, X_test_proc, y_train, y_test))\n",
    "\n",
    "# Modelo 4: LightGBM optimizado\n",
    "modelos_resultados.append(evaluar_modelo_full(\"LightGBM\", best_lgb, X_train_proc, X_test_proc, y_train, y_test))\n",
    "\n",
    "# Modelo 5: Random Forest con GridSearchCV\n",
    "modelos_resultados.append(evaluar_modelo_full(\"RandomForest (tuned)\", best_rf, X_train_proc, X_test_proc, y_train, y_test))\n",
    "\n",
    "# Crear tabla resumen\n",
    "tabla_resultados = pd.DataFrame(modelos_resultados)\n",
    "\n",
    "# Ordenar por RMSE_log ascendente (mejor modelo arriba)\n",
    "tabla_resultados = tabla_resultados.sort_values(by=\"RMSE_log\")\n",
    "\n",
    "# Mostrar con 3 decimales\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:,.3f}\")\n",
    "display(tabla_resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d35b5ae",
   "metadata": {},
   "source": [
    "Cross Validation original con los tres modelos haciendo fit transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb4ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# =====================\n",
    "# 1) Preprocesar con float32 para reducir memoria\n",
    "# =====================\n",
    "X_train_proc = preprocessor.fit_transform(X_train).astype(np.float32)\n",
    "X_test_proc  = preprocessor.transform(X_test).astype(np.float32)\n",
    "\n",
    "# =====================\n",
    "# 2) Funci√≥n para evaluaci√≥n r√°pida\n",
    "# =====================\n",
    "def evaluar_modelo_simple(modelo, X_train, y_train, X_test, y_test, nombre=\"Modelo\"):\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_test)\n",
    "\n",
    "    mae_log = mean_absolute_error(y_test, y_pred)\n",
    "    rmse_log = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Revertimos log a pesos\n",
    "    y_test_original = np.exp(y_test)\n",
    "    y_pred_original = np.exp(y_pred)\n",
    "    mae_pesos = mean_absolute_error(y_test_original, y_pred_original)\n",
    "    rmse_pesos = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n",
    "\n",
    "    return {\n",
    "        \"Modelo\": nombre,\n",
    "        \"R2\": r2,\n",
    "        \"MAE_log\": mae_log,\n",
    "        \"RMSE_log\": rmse_log,\n",
    "        \"MAE_pesos\": mae_pesos,\n",
    "        \"RMSE_pesos\": rmse_pesos\n",
    "    }\n",
    "\n",
    "# =====================\n",
    "# 3) RandomizedSearchCV para cada modelo\n",
    "# =====================\n",
    "\n",
    "# --- Random Forest ---\n",
    "param_rf = {\n",
    "    \"n_estimators\": [200, 400, 600],\n",
    "    \"max_depth\": [10, 15, 20, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "search_rf = RandomizedSearchCV(rf, param_rf, n_iter=10, cv=3, scoring='r2',\n",
    "                               verbose=2, random_state=42, n_jobs=-1)\n",
    "t0 = time.time()\n",
    "search_rf.fit(X_train_proc, y_train)\n",
    "print(\"Random Forest mejores params:\", search_rf.best_params_, \"Tiempo:\", round(time.time()-t0, 2), \"s\")\n",
    "\n",
    "# --- XGBoost ---\n",
    "param_xgb = {\n",
    "    \"n_estimators\": [200, 400, 600],\n",
    "    \"max_depth\": [3, 5, 7, 10],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"subsample\": [0.7, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 1.0]\n",
    "}\n",
    "xgb = XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1)\n",
    "search_xgb = RandomizedSearchCV(xgb, param_xgb, n_iter=10, cv=3, scoring='r2',\n",
    "                                verbose=2, random_state=42, n_jobs=-1)\n",
    "t0 = time.time()\n",
    "search_xgb.fit(X_train_proc, y_train)\n",
    "print(\"XGBoost mejores params:\", search_xgb.best_params_, \"Tiempo:\", round(time.time()-t0, 2), \"s\")\n",
    "\n",
    "# --- LightGBM ---\n",
    "param_lgb = {\n",
    "    \"n_estimators\": [200, 400, 600],\n",
    "    \"max_depth\": [-1, 10, 15, 20],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"subsample\": [0.7, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 1.0]\n",
    "}\n",
    "lgb = LGBMRegressor(objective='regression', random_state=42, n_jobs=-1)\n",
    "search_lgb = RandomizedSearchCV(lgb, param_lgb, n_iter=10, cv=3, scoring='r2',\n",
    "                                verbose=2, random_state=42, n_jobs=-1)\n",
    "t0 = time.time()\n",
    "search_lgb.fit(X_train_proc, y_train)\n",
    "print(\"LightGBM mejores params:\", search_lgb.best_params_, \"Tiempo:\", round(time.time()-t0, 2), \"s\")\n",
    "\n",
    "# =====================\n",
    "# 4) Evaluar y comparar\n",
    "# =====================\n",
    "resultados = []\n",
    "resultados.append(evaluar_modelo_simple(search_rf.best_estimator_, X_train_proc, y_train, X_test_proc, y_test, \"Random Forest\"))\n",
    "resultados.append(evaluar_modelo_simple(search_xgb.best_estimator_, X_train_proc, y_train, X_test_proc, y_test, \"XGBoost\"))\n",
    "resultados.append(evaluar_modelo_simple(search_lgb.best_estimator_, X_train_proc, y_train, X_test_proc, y_test, \"LightGBM\"))\n",
    "\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "print(\"\\nüìä Comparativa de modelos:\")\n",
    "print(df_resultados)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
