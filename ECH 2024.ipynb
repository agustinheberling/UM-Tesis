{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d08fe1d-250f-45ed-bc5e-bcba8bbb022a",
   "metadata": {},
   "source": [
    "**Analisis de la Encuesta Continua de Hogares 2024**\n",
    "\n",
    "- Dataset: https://www4.ine.gub.uy/Anda5/index.php/catalog/767/get-microdata\n",
    "- Diccionario: https://www4.ine.gub.uy/Anda5/index.php/catalog/767/data-dictionary/F4?file_name=ECH_implantacion_2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919eed26-0d7d-45f7-84f6-0d51c12a510d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.11.5)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# CARGO EL CSV CON PANDAS\n",
    "import pandas as pd\n",
    "ech = pd.read_csv(r'C:\\Users\\ut603933\\UM\\tesis\\ECH_implantacion_2024.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d834bdc-304f-4583-a585-54c53735a485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFORMACION DEL DATASET\n",
    "# print(\"Informaci√≥n del dataset de entrenamiento:\")\n",
    "# print(ech.info())\n",
    "#print(\"\\nResumen estad√≠stico:\")\n",
    "#print(ech.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997fa417-59f9-497a-a207-d5b029321d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VERIFICAR VALORES NULOS\n",
    "# print(\"Columnas con valores nulos (por nombre de la variable):\")\n",
    "# print(ech.isnull().sum()[ech.isnull().sum() > 0].sort_index())\n",
    "#print(\"Columnas con valores nulos (ordenadas):\")\n",
    "#print(ech.isnull().sum()[ech.isnull().sum() > 0].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fdac0d-c3a2-4236-9614-ce915e50d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tabulate import tabulate\n",
    "# print(tabulate(ech, headers = 'keys', tablefmt = 'psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2e37de-7a5b-4a61-bc69-50f9d985542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOSTRAR EL DF\n",
    "# pd.set_option('display.max_rows', None)  # Show all rows\n",
    "# pd.set_option('display.max_columns', 10)  # Show all columns\n",
    "# pd.set_option('display.width', 100)  # Adjust display width\n",
    "# pd.set_option('display.max_colwidth', None)  # Show full column contents\n",
    "# print(ech)\n",
    "# #pd.reset_option('all') # Restore default option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9a78a7-ba37-46e3-a21c-bd217a8d63b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ech.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c36bb8b-f586-4295-9be4-0fcce03ace64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTO LIBRERIAS\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53652266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Informaci√≥n general del dataset ===\n",
    "print(\"Dimensiones del dataset:\", ech.shape)\n",
    "print(\"\\nPrimeras filas:\")\n",
    "display(ech.head())\n",
    "\n",
    "print(\"\\nTipos de datos:\")\n",
    "display(ech.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1f2140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar todas las columnas sin truncar\n",
    "pd.set_option('display.max_columns', None)  # muestra todas las columnas\n",
    "pd.set_option('display.expand_frame_repr', False)  # evita salto de l√≠nea\n",
    "\n",
    "# Mostrar todos los nombres de columna\n",
    "print(ech.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae81d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver si hay columnas duplicadas por nombre\n",
    "duplicated_cols = ech.columns[ech.columns.duplicated()].tolist()\n",
    "print(\"Columnas duplicadas por nombre:\", duplicated_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a1d717",
   "metadata": {},
   "source": [
    "No hay columnas duplicadas en el dataset original. Las columnas que aparecen duplicadas y no permiten ejecutar el pipeline las repetimos erroneamente al categorizar las variables producto de que si aparecen duplicadas en el Excel del diccionario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54814d06",
   "metadata": {},
   "source": [
    "## Analisis exploratorio de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c84aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. An√°lisis de valores nulos ===\n",
    "nulos = ech.isnull().sum()\n",
    "nulos = nulos[nulos > 0].sort_values(ascending=False)\n",
    "print(\"\\nColumnas con nulos (cantidad):\")\n",
    "display(nulos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e56fb87",
   "metadata": {},
   "source": [
    "Verificando el diccionario, los valores nulos son para las variables:\n",
    "- c5_13: PROBLEMAS DE LA VIVIENDA Pisos y muros agrietados\n",
    "- d181, d181_b, d184, d184_1, d184_b, d229, d230, d231, d232: SERVICIO DOMESTICO\n",
    "- f288, f289, f290, f291_a, f291_b, f292, f293, f294, f295: Lugar y modalidad de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bbb696",
   "metadata": {},
   "outputs": [],
   "source": [
    "nulos_pct = ech.isnull().mean().sort_values(ascending=False)\n",
    "display(nulos_pct[nulos_pct > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5241e126",
   "metadata": {},
   "source": [
    "Las preguntas son de los modulos:\n",
    "-   Servicio dom√©stico ‚Üí s√≥lo hogares que contratan personal.\n",
    "-   Lugar y modalidad de trabajo ‚Üí s√≥lo personas ocupadas.\n",
    "\n",
    "Confirmamos que son variables condicionales por dise√±o: \n",
    "-   La mitad de la poblaci√≥n no responde esas preguntas porque no aplica.\n",
    "-   El 50% restante s√≠ tiene datos v√°lidos y √∫tiles.\n",
    "\n",
    "No eliminamos esas columnas por tener ‚Äúdemasiados nulos‚Äù, ya que no se trata de datos ausentes por error, sino de l√≥gica del cuestionario.\n",
    "\n",
    "Podemos usarlas de forma segmentada, por ejemplo:\n",
    "-   Hacer modelos distintos para personas ocupadas (que tienen datos en f288‚Äìf295) y no ocupadas, o\n",
    "-   Incluir una variable indicadora de ‚Äúaplica o no aplica‚Äù para evitar que el modelo se confunda.\n",
    "\n",
    "Son una se√±al indirecta de pertenencia a ciertos grupos: tener datos en esas columnas podr√≠a implicar estar ocupado o tener personal a cargo ‚Üí lo que s√≠ puede estar correlacionado con el ingreso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ff227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3. Distribuci√≥n de la variable objetivo ===\n",
    "# Suponemos que la variable objetivo es 'YDA'\n",
    "target = 'YDA'\n",
    "print(\"\\nEstad√≠sticas de ingreso (YDA):\")\n",
    "display(ech[target].describe())\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(ech[target], bins=50, kde=True)\n",
    "plt.title('Distribuci√≥n del Ingreso Disponible Ajustado (YDA)')\n",
    "plt.xlabel('Ingreso')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962ac456",
   "metadata": {},
   "source": [
    "Se observa que:\n",
    "- Muy asim√©trica (sesgo a la derecha).\n",
    "- Muchos ingresos entre $0 y $200.000.\n",
    "- Algunos valores extremos que superan el mill√≥n (outliers evidentes).\n",
    "\n",
    "Tratamos a continuacion: \n",
    "- Usmos la variable log_YDA como target en modelos lineales y √°rboles si queremos reducir la sensibilidad a los valores extremos.\n",
    "- Visualizamos un boxplot ya que el max es 25 veces el 75¬∫ percentil.\n",
    "- Vemos valores en 0 ya que min = 0 ‚Üí ¬øIngreso nulo o inactivo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fd3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target en modelos lineales y √°rboles si queremos reducir la sensibilidad a los valores extremos\n",
    "ech['log_YDA'] = np.log1p(ech['YDA'])  # log(1 + YDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d35503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizar con boxplot y/o stripplot\n",
    "sns.boxplot(x=ech['YDA'])\n",
    "plt.title(\"Boxplot de ingreso (YDA)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd4f485",
   "metadata": {},
   "source": [
    "La gr√°fica muestra claramente lo que ya anticipamos:\n",
    "- Una alta concentraci√≥n de valores en el rango bajo (entre $0 y $200.000).\n",
    "- Una larga cola derecha de outliers extremos, que se extienden hasta los 2.5 millones.\n",
    "\n",
    "üîç Esto confirma que:\n",
    "- El ingreso en la ECH 2024 est√° fuertemente sesgado.\n",
    "- La dispersi√≥n es muy alta.\n",
    "- Los outliers son numerosos pero no necesariamente errores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946b41d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ver valores en cero\n",
    "ech[ech['YDA'] == 0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e897e1",
   "metadata": {},
   "source": [
    "Esto significa que solo 12 personas tienen ingreso 0 declarado, lo cual:\n",
    "- ‚úÖ No es un problema de datos generalizado, ni requiere imputaci√≥n.\n",
    "- ‚ö†Ô∏è Pero s√≠ vale la pena entender el perfil de estas 12 personas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec1319",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ech[ech['YDA'] == 0][['e27', 'e26', 'f269', 'd8_1', 'd9', 'c1']])  # o las columnas que representen edad, ocupaci√≥n, tipo de hogar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ab3869",
   "metadata": {},
   "source": [
    "La mayoria inquilinos o arrendatarios (d8_1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5beb52a",
   "metadata": {},
   "source": [
    "La distribuci√≥n de YDA:\n",
    "- Se comporta como es esperable en datos de ingreso.\n",
    "- No requiere imputaci√≥n de nulos en YDA (no hay).\n",
    "\n",
    "‚úÖ Conclusiones clave:\n",
    "- Los valores extremos est√°n presentes, pero no dominan.\n",
    "- No es necesario eliminarlos todav√≠a. Algunos modelos (como √°rboles) los manejan bien.\n",
    "\n",
    "Transformaci√≥n logar√≠tmica es aconsejable.\n",
    "Especialmente para modelos sensibles a la escala o distribuci√≥n (Regresi√≥n Lineal, Redes Neuronales).\n",
    "\n",
    "Pocos casos con ingreso cero.\n",
    "Son casos especiales que pod√©s dejar o filtrar si hac√©s segmentaci√≥n (ej. \"ocupados\", \"con ingreso distinto de cero\", etc.).\n",
    "\n",
    "S√≠ se beneficia de:\n",
    "- Log-transformaci√≥n (log1p).\n",
    "- Revisi√≥n de outliers extremos.\n",
    "- Segmentaci√≥n posterior por tipo de poblaci√≥n (ocupada/no ocupada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a992f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. Identificar variables num√©ricas y categ√≥ricas ===\n",
    "num_vars = ech.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_vars = ech.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Eliminar la variable target de ambas listas si est√° incluida\n",
    "if target in num_vars: num_vars.remove(target)\n",
    "if target in cat_vars: cat_vars.remove(target)\n",
    "\n",
    "print(f\"Variables num√©ricas: {len(num_vars)}\")\n",
    "print(f\"Variables categ√≥ricas: {len(cat_vars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966a8ddf",
   "metadata": {},
   "source": [
    "Muchas variables categ√≥ricas est√°n codificadas num√©ricamente, por ejemplo:\n",
    "- sexo: 1 = Hombre, 2 = Mujer\n",
    "- estado civil actual: 1 = Separado, 2 = Divorciado, 3 = Casado, etc.\n",
    "- nivel que esta cursando: 4 = Educacion media basica, 6 = Educacion media superior, etc.\n",
    "\n",
    "Estas se almacenan como int64 o float64, por lo que pandas las interpreta como num√©ricas, cuando en realidad son categ√≥ricas ordinales o nominales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4eb5f9",
   "metadata": {},
   "source": [
    "No imputamos variables numericas ni categoricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fdc5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 5. Tratamiento de valores nulos ===\n",
    "# Para num√©ricas: imputar con la mediana\n",
    "#for col in num_vars:\n",
    "#    if ech[col].isnull().any():\n",
    "#        ech[col] = ech[col].fillna(ech[col].median())\n",
    "\n",
    "# Para categ√≥ricas: imputar con una categor√≠a especial\n",
    "#for col in cat_vars:\n",
    "#    if ech[col].isnull().any():\n",
    "#        ech[col] = ech[col].fillna('No informado')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ed66e",
   "metadata": {},
   "source": [
    "Posible clasificacion de variables categoricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef0ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. Codificaci√≥n de variables categ√≥ricas ===\n",
    "# Convertir binarias codificadas como 1/2 o S√≠/No a 0/1\n",
    "#def codificar_binarias(df):\n",
    "#    for col in df.columns:\n",
    "#        if df[col].nunique() == 2:\n",
    "#            vals = sorted(df[col].dropna().unique())\n",
    "#            if set(vals) <= set([0,1,2]):\n",
    "#                df[col] = df[col].map({vals[0]: 0, vals[1]: 1})\n",
    "#    return df\n",
    "\n",
    "#ech = codificar_binarias(ech)\n",
    "\n",
    "# One-hot encoding para variables categ√≥ricas no binarias\n",
    "#ech_encoded = pd.get_dummies(ech, columns=cat_vars, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7927837f",
   "metadata": {},
   "source": [
    "Imputaci√≥n condicional basada en la l√≥gica del cuestionario:\n",
    "\n",
    "Si PERSONAS PAGAS PARA REALIZAR TAREAS DOM√âSTICAS O DE CUIDADOS d181 (*) 1 = S√≠ / 2 = No es 2 = No entonces:\n",
    "- PERSONAS CONTRATADAS PARA REALIZAR TAREAS  DOM√âSTICAS O DE CUIDADO d229 (*) N¬∞ Cantidad de personas contratadas es 0.\n",
    "- CANTIDAD DE HORAS TRABAJADAS A LA SEMANA d230 (*) N¬∞ Cantidad de horas habitualmente trabajadas en la semana es 0.\n",
    "- PERNOCTA EN EL HOGAR d231 (*) 1 = S√≠ / 2 = No es 2 = No.\n",
    "- CANTIDAD DE PERSONAS QUE PERNOCTAN d232 (*) N¬∞ Cantidad de personas que pernoctan en el hogar es 0.\n",
    "\n",
    "Si AYUDA GRATUITA DE OTROS FAMILIARES QUE NO INTEGRAN EL HOGAR d184 (*) 1 = S√≠ / 2 = No es 2 = No entonces:\n",
    "- CANTIDAD DE HORAS TRABAJADAS A LA SEMANA d184_1 (*) N¬∞ Cantidad de horas habitualmente trabajadas en la semana es 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b185f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si d181 == 2 (no tiene servicio dom√©stico), completar columnas relacionadas\n",
    "\n",
    "# Estas deben ser 0 si est√°n vac√≠as\n",
    "for col in ['d229', 'd230', 'd232']:\n",
    "    ech.loc[(ech['d181'] == 2) & (ech[col].isnull()), col] = 0\n",
    "\n",
    "# Esta debe ser 2 si est√° vac√≠a (respuesta negativa expl√≠cita)\n",
    "ech.loc[(ech['d181'] == 2) & (ech['d231'].isnull()), 'd231'] = 2\n",
    "\n",
    "# Si d184 == 2 (no tiene ayuda gratuita), entonces d184_1 debe ser 0 si est√° vac√≠a\n",
    "ech.loc[(ech['d184'] == 2) & (ech['d184_1'].isnull()), 'd184_1'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc90c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificaci√≥n de valores nulos despu√©s de imputaci√≥n corregida\n",
    "cols_verificar = ['d229', 'd230', 'd231', 'd232', 'd184_1']\n",
    "\n",
    "print(\"Verificaci√≥n de valores nulos en variables imputadas:\")\n",
    "print(ech[cols_verificar].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b606a1",
   "metadata": {},
   "source": [
    "Si f269 TRABAJO LA SEMANA ANTERIOR es 2 = No imputamos las siguientes variables tambien como 2 = No:\n",
    "- f290 USO DE PC, TEL√âFONO INTELIGENTE O TABLETA PARA TRABAJAR\n",
    "- f291_a OTRO LUGAR DIFERENTE AL HOGAR, LOCAL PROPIO U OFICINA O INSTALACI√ìN DEL CLIENTE\n",
    "- f291_b TRABAJO EN UN LUGAR DIFERENTE AL HOGAR O AL LUGAR HABITUAL DE TRABAJO\n",
    "- f292 TRABAJO REALIZADO FUERA DE SUS PROPIAS INSTALACIONES\n",
    "- f295 TRABAJO SEMANAL EN 2 LUGARES DIFRENTES A LAS INSTALACIONES DEL EMPLEADOR/A O PROPIO LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0ab55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_f269 = ['f290', 'f291_a', 'f291_b', 'f292', 'f293', 'f295']\n",
    "for col in cols_f269:\n",
    "    ech.loc[(ech['f269'] == 2) & (ech[col].isnull()), col] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5368ec5",
   "metadata": {},
   "source": [
    "Unificacion de variables d181 con d181_b y d184 con d184_b:\n",
    "\n",
    "- d181 y d181_b miden lo mismo (\"personas pagas para tareas dom√©sticas o de cuidados\"), pero en distintos semestres.\n",
    "- d184 y d184_b tambi√©n miden lo mismo (\"ayuda gratuita de familiares fuera del hogar\").\n",
    "- Ambas parejas de variables tienen 50% de valores nulos complementarios, por lo que es totalmente v√°lido consolidarlas en una nueva variable que conserve el significado original.\n",
    "\n",
    "Para cada par, usamos el valor no nulo que exista.\n",
    "\n",
    "Si los dos valores son nulos (por ejemplo, por no aplicar), la variable consolidada tambi√©n quedar√° nula.\n",
    "\n",
    "Primero recodificamos d181_b y d184_b a binario, ya que toman valores 1, 2 y 3 para Si y 1 y 2 para Si respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52650378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contamos cu√°ntos ceros hay en d181 y d184 antes de imputar\n",
    "n_d181_ceros = (ech['d181'] == 0).sum()\n",
    "n_d184_ceros = (ech['d184'] == 0).sum()\n",
    "\n",
    "print(f\"Cantidad de ceros en d181: {n_d181_ceros}\")\n",
    "print(f\"Cantidad de ceros en d184: {n_d184_ceros}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc10775",
   "metadata": {},
   "source": [
    "Imputaci√≥n de valores 0 en d181 y d184:\n",
    "\n",
    "Durante el procesamiento inicial, se imputaron valores 0 en d181 y d184 en casos donde no se respondi√≥ la pregunta sobre servicio dom√©stico o ayuda gratuita.\n",
    "\n",
    "Estos 0 representan casos no aplicables (por ejemplo, hogares sin servicio dom√©stico), por lo tanto, decidimos reemplazarlos por el valor 2 (No) antes de crear las columnas unificadas d181_unificado y d184_unificado.\n",
    "\n",
    "Esto evita que la unificaci√≥n resulte en valores faltantes injustificados y preserva la l√≥gica del cuestionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee6c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazamos 0 por 2 (No aplica) en las variables d181 y d184 antes de unificarlas\n",
    "ech.loc[ech['d181'] == 0, 'd181'] = 2\n",
    "ech.loc[ech['d184'] == 0, 'd184'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee6c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recodificar d181_b a binario (1 = S√≠, 2 = No)\n",
    "d181_b_bin = ech['d181_b'].map({1: 1, 2: 1, 3: 1, 4: 2})\n",
    "\n",
    "# Unificar d181 y d181_b\n",
    "ech['d181_unificado'] = ech['d181'].combine_first(d181_b_bin)\n",
    "\n",
    "# Recodificar d184_b a binario (1 = S√≠, 2 = No)\n",
    "d184_b_bin = ech['d184_b'].map({1: 1, 2: 1, 3: 2})\n",
    "\n",
    "# Unificar d184 y d184_b\n",
    "ech['d184_unificado'] = ech['d184'].combine_first(d184_b_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172171cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C√≥digo de validaci√≥n para las columnas unificadas\n",
    "# Validar que no haya valores nulos\n",
    "print(\"Valores nulos en columnas unificadas:\")\n",
    "print(ech[['d181_unificado', 'd184_unificado']].isnull().sum())\n",
    "\n",
    "# Validar que solo haya valores 1 o 2\n",
    "print(\"\\nValores √∫nicos en d181_unificado:\")\n",
    "print(ech['d181_unificado'].value_counts(dropna=False).sort_index())\n",
    "\n",
    "print(\"\\nValores √∫nicos en d184_unificado:\")\n",
    "print(ech['d184_unificado'].value_counts(dropna=False).sort_index())\n",
    "\n",
    "# Validar proporci√≥n de respuestas \"S√≠\" y \"No\"\n",
    "print(\"\\nProporci√≥n de respuestas (1 = S√≠, 2 = No):\")\n",
    "print(\"d181_unificado:\")\n",
    "print(ech['d181_unificado'].value_counts(normalize=True).map(lambda x: f\"{x:.2%}\"))\n",
    "\n",
    "print(\"d184_unificado:\")\n",
    "print(ech['d184_unificado'].value_counts(normalize=True).map(lambda x: f\"{x:.2%}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e35430",
   "metadata": {},
   "source": [
    "Esto implica que solo 8 de cada 100 hogares pagan a una persona por tareas dom√©sticas o cuidados. Esto es completamente razonable en el contexto uruguayo, dado que este tipo de contrataci√≥n suele estar concentrada en hogares de mayor poder adquisitivo o con necesidades espec√≠ficas de cuidado. Son coherentes con lo esperado para la poblaci√≥n general en una encuesta nacional.\n",
    "\n",
    "Menos del 5% de los hogares reciben ayuda gratuita de otros familiares que no integran el hogar. Tambi√©n es razonable. La solidaridad familiar informal existe, pero no es una pr√°ctica diaria ni generalizada como para que aparezca en un alto porcentaje en una encuesta estructurada.\n",
    "\n",
    "Ambas proporciones:\n",
    "- Son coherentes con lo esperado para la poblaci√≥n general en una encuesta nacional.\n",
    "- Validan que la l√≥gica de unificaci√≥n y recodificaci√≥n fue correcta.\n",
    "- No presentan valores an√≥malos o distribuciones sesgadas artificialmente (como hubiera pasado si imputaras todo como 2 sin justificaci√≥n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b039a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las columnas originales si ya no las necesitamos\n",
    "ech.drop(columns=['d181', 'd181_b', 'd184', 'd184_b'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8648d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7. Correlaci√≥n con la variable objetivo (opcional visualizaci√≥n) ===\n",
    "# Seleccionamos solo las columnas num√©ricas para evitar errores\n",
    "ech_numericas = ech.select_dtypes(include=['number'])\n",
    "\n",
    "# Calculamos correlaciones solo sobre esas columnas\n",
    "correlaciones = ech_numericas.corr()[target].drop(target).sort_values(ascending=False)\n",
    "\n",
    "# Mostramos resultados\n",
    "print(\"\\nTop 10 variables correlacionadas con el ingreso (YDA):\")\n",
    "display(correlaciones.head(10))\n",
    "\n",
    "# Graficamos\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=correlaciones.head(10).values, y=correlaciones.head(10).index)\n",
    "plt.title(\"Top 10 variables correlacionadas con el ingreso\")\n",
    "plt.xlabel(\"Correlaci√≥n\")\n",
    "plt.ylabel(\"Variable\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2fc772",
   "metadata": {},
   "source": [
    "Verificando el diccionario, las variables correlacionadas con el ingreso son:\n",
    "- HT11: INGRESO TOTAL DEL HOGAR CON VALOR LOCATIVO SIN SERVICIO DOM√âSTICO\n",
    "- YDA_SVL: INGRESO DISPONIBLE AJUSTADO (SIN VALOR LOCATIVO)\n",
    "- YSVL: INGRESO TOTAL DEL HOGAR SIN VALOR LOCATIVO SIN SERVICIO DOM√âSTICO\n",
    "- d8_3: TENENCIA DE LA VIVIENDA > Monto del alquiler (efectivamente pagado o estimado)\n",
    "- eg_ps2: MONTO M√çNIMO MENSUAL REQUERIDO PARA SATISFACER LAS NECESIDADES B√ÅSICAS\n",
    "- HT13: VALOR LOCATIVO\n",
    "- d14: CANTIDAD DE BA√ëOS\n",
    "- d21_15_4: ELEMENTOS DE CONFORT > Cantidad de microcomputadores que no son del Plan Ceibal\n",
    "- PT1: TOTAL DE INGRESOS PERSONALES\n",
    "- d21_14_1: ELEMENTOS DE CONFORT > Cantidad de equipos de aire acondicionado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a26385",
   "metadata": {},
   "source": [
    "Profundizamos en la correlacion con la variable objetivo:\n",
    "- Usar correlaci√≥n de Pearson para variables num√©ricas ‚Üí detectar alta correlaci√≥n con YDA o log_YDA.\n",
    "- Usar feature importance r√°pida con un modelo tipo RandomForestRegressor para detectar tambi√©n correlaci√≥n no lineal o relaciones m√°s complejas.\n",
    "- Generar un listado de columnas candidatas a eliminar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c3bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BLOQUE 0 - Preparaci√≥n\n",
    "# =========================\n",
    "df_corr_rf = ech.copy()  # Usa tu DataFrame original con las 535 variables\n",
    "target_var = 'YDA'\n",
    "\n",
    "# Eliminar filas donde el target sea NaN\n",
    "df_corr_rf = df_corr_rf[df_corr_rf[target_var].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcde780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BLOQUE 1 - Correlaci√≥n\n",
    "# =========================\n",
    "# Selecciona variables num√©ricas reales (excluyendo el target)\n",
    "num_vars = df_corr_rf.select_dtypes(include=[np.number]).columns.drop(target_var, errors='ignore')\n",
    "\n",
    "correlaciones = df_corr_rf[num_vars].corrwith(df_corr_rf[target_var]).sort_values(key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nüìå Correlaci√≥n de Pearson con YDA (top 20):\")\n",
    "print(correlaciones.head(20))\n",
    "\n",
    "# Variables con correlaci√≥n alta (umbral configurable, ej. >= 0.85)\n",
    "umbral_corr = 0.85\n",
    "vars_corr_altas = correlaciones[correlaciones.abs() >= umbral_corr].index.tolist()\n",
    "\n",
    "print(f\"\\nVariables con correlaci√≥n |r| >= {umbral_corr}:\")\n",
    "print(vars_corr_altas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1855eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# =========================\n",
    "# BLOQUE 2 - Importancia con Random Forest\n",
    "# =========================\n",
    "if len(num_vars) > 0:\n",
    "    print(\"\\nüìå Entrenando Random Forest para evaluar importancia de variables...\")\n",
    "    rf = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "    rf.fit(df_corr_rf[num_vars], df_corr_rf[target_var])\n",
    "\n",
    "    importancias_rf = pd.Series(rf.feature_importances_, index=num_vars).sort_values(ascending=False)\n",
    "\n",
    "    print(\"\\nüìå Importancia seg√∫n Random Forest (top 20):\")\n",
    "    print(importancias_rf.head(20))\n",
    "\n",
    "    # Variables con importancia muy alta (ej. >= 0.05)\n",
    "    umbral_importancia = 0.05\n",
    "    vars_importantes_rf = importancias_rf[importancias_rf >= umbral_importancia].index.tolist()\n",
    "\n",
    "    print(f\"\\nVariables con importancia >= {umbral_importancia}:\")\n",
    "    print(vars_importantes_rf)\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No hay variables num√©ricas para entrenar el Random Forest.\")\n",
    "    importancias_rf = pd.Series(dtype=float)\n",
    "    vars_importantes_rf = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4e4227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# BLOQUE 3 - Unificaci√≥n de listas\n",
    "# =========================\n",
    "vars_a_eliminar = list(set(vars_corr_altas + vars_importantes_rf))\n",
    "print(f\"\\nüìå Total variables candidatas a eliminar por alta relaci√≥n con {target_var}: {len(vars_a_eliminar)}\")\n",
    "print(vars_a_eliminar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322fe7b7",
   "metadata": {},
   "source": [
    "Preprocesamiento de variables categoricas dicotomicas, nominales u ordinales y numericas discretas o continuas. \n",
    "\n",
    "No incluimos:\n",
    "- HT11: INGRESO TOTAL DEL HOGAR CON VALOR LOCATIVO SIN SERVICIO DOM√âSTICO\n",
    "- YSVL: INGRESO TOTAL DEL HOGAR SIN VALOR LOCATIVO SIN SERVICIO DOM√âSTICO\n",
    "- YDA_SVL: INGRESO DISPONIBLE AJUSTADO (SIN VALOR LOCATIVO)\n",
    "- YDA: INGRESO DISPONIBLE AJUSTADO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361eb39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== PASO 1: Definir las listas de variables seg√∫n nuestro Excel ======\n",
    "\n",
    "dicotomicas = ['c5_2', 'c5_10', 'c5_11', 'c5_12', 'c5_13', 'c6', 'd8_4', 'd15', 'd21_1', 'd21_2', 'd21_3', 'd21_6', 'd21_4', 'd21_5', 'd21_20', 'd21_7', 'd21_10', 'd21_11', 'd21_12', 'd21_13', 'd21_14', 'd21_15', 'd21_15_1', 'd21_15_3', 'd21_15_5', 'd21_16', 'd21_16_1', 'd21_16_2', 'd21_21', 'd21_17', 'd21_18', 'd21_19', 'd181_unificado', 'd231', 'd184_unificado', 'e26', 'e29_1', 'e29_2', 'e29_3', 'e29_4', 'e29_5', 'e31', 'e32', 'e33', 'e185', 'e38', 'e45_4_1_cv', 'e45_cvb', 'e46_cv', 'e48', 'e581a', 'e582', 'e197_1', 'e201_1a', 'e201_1b', 'e201_1c', 'e201_1d', 'e51_6b', 'e215_1', 'e218_1', 'e221_1', 'e224_1', 'e559', 'e584', 'e59', 'f269', 'f270', 'f271', 'f272', 'f274', 'f276', 'f82', 'f84', 'f278', 'f280_1', 'f280_2', 'f280_3', 'f281_1', 'f281_2', 'f281_3', 'f281_4', 'f75', 'f81', 'f266_2', 'f267', 'f268', 'f289', 'f290', 'f291_a', 'f291_b', 'f292', 'f293', 'f294', 'f295', 'f96', 'f99', 'f100', 'f102', 'f298', 'f299', 'f114', 'f115', 'f300', 'f116', 'f117', 'f123', 'f124_1', 'f124_2', 'f124_3', 'f124_5', 'g250_1', 'g250_2', 'g250_3', 'g250_4', 'g250_5', 'g127', 'g128', 'g129', 'g129_1', 'g130', 'g131', 'g133', 'g_st_1', 'g251_1', 'g251_2', 'g251_3', 'g251_4', 'g251_5', 'g135', 'g136', 'g137', 'g137_1', 'g138', 'g139', 'g141', 'g_itnd_1', 'g142', 'g_itnd_2', 'g144', 'g_it_1', 'g_it_2', 'g149', 'g149_1', 'g150', 'g255', 'g152', 'g153', 'g258', 'g154', 'h155', 'h156', 'h272', 'h273', 'h274', 'h252', 'h159', 'h160', 'h161', 'h162', 'h227', 'h269', 'h167_2', 'h167_3', 'h167_4', 'h169', 'h271', 'h171', 'h172', 'h173', 'i228', 'i259', 'eg_ps1', 'SUBEMPLEO', 'pobre06', 'indig06', 'pobre_multi', 'pobre17', 'indig17']\n",
    "\n",
    "nominales = ['dpto', 'nom_dpto', 'secc', 'ESTRED13', 'LOC_AGR_13', 'NOM_LOC_AGR_13', 'c1', 'd8_1', 'd11', 'd12', 'e557', 'e29_6', 'e30', 'e35', 'e36', 'e37', 'e234_2', 'e39', 'e235_2', 'e236', 'e236_4', 'e45_cv', 'e45_1_1_cv', 'e45_2_1_cv', 'e45_3_1_cv', 'e45_cva', 'e47_cv', 'e190', 'e49', 'e581', 'e209_1', 'e202', 'e214_1', 'e217_1', 'e220_1', 'e223_1', 'e226_1', 'e246', 'f273', 'f69', 'f69_1', 'f277', 'f71_2', 'f72_2', 'f73', 'f83', 'f278_a', 'f279', 'f76_2', 'f266', 'f266_1', 'f305', 'f306', 'f78', 'f80', 'f285', 'f286', 'f287', 'f288', 'f90_2', 'f91_2', 'f92', 'f97', 'f94', 'f101', 'f103', 'f104', 'f110', 'f111', 'f108', 'f301', 'f106', 'f122', 'f119_2', 'f120_2', 'f121', 'f125', 'g132', 'g140', 'g_itnd_3', 'g256', 'POBPCOAC', 'USO_RRAA']\n",
    "\n",
    "ordinales = ['region', 'REGION_4', 'c2', 'c3', 'c4', 'd13', 'd16', 'd18', 'd260', 'd19', 'd20', 'e579', 'e583', 'e579a', 'f275', 'f283', 'f77', 'f93', 'h167_1', 'eg_ahorro', 'eg_ps3', 'eg_ps4', 'eg_ps5', 'eg_ps6', 'eg_ps7', 'eg_ps8']\n",
    "\n",
    "numericas_discretas = ['ID', 'nper', 'anio', 'mes', 'GR', 'ccz', 'barrio', 'c6_1', 'd9', 'd10', 'd14', 'd21_4_1', 'd21_5_1', 'd21_14_1', 'd21_15_2', 'd21_15_4', 'd21_15_6', 'd21_18_1', 'd21_19_1', 'd229', 'd230', 'd232', 'd184_1', 'd23', 'd24', 'd25', 'e27', 'e31_1', 'e32_1', 'e34', 'e186_1', 'e186_2', 'e186_3', 'e186_4', 'e37_2', 'e38_1', 'e39_2', 'e236_2', 'e45_1_1_1_cv', 'e45_2_1_1_cv', 'e45_3_1_1_cv', 'e45_4_1_1_cv', 'e47_1_cv', 'e49a', 'e582_1', 'e582_2', 'e582_3', 'e51_2', 'e51_3', 'e51_4_a', 'e51_4_b', 'e51_5', 'e51_6', 'e51_6a', 'e51_8', 'e51_9', 'e51_10', 'e51_11', 'e559_1', 'e559_2', 'e247', 'f70', 'f80_2', 'f284_1', 'f284_2', 'f284_3', 'f284_4', 'f284_5', 'f284_6', 'f284_7', 'f85', 'f307', 'f308', 'f94_2', 'f296_1', 'f296_2', 'f296_3', 'f296_4', 'f296_5', 'f296_6', 'f296_7', 'f98', 'f297', 'f113', 'f118_1', 'f118_2', 'g127_1', 'g127_2', 'g132_1', 'g132_2', 'g132_3', 'g135_1', 'g135_2', 'g140_1', 'g140_2', 'g140_3', 'g151_6', 'g151_3', 'g151_4', 'h272_1', 'h273_1', 'h274_1', 'h274_2', 'h274_3', 'h158_1', 'h158_2', 'h171_2', 'HT19']\n",
    "\n",
    "numericas_continuas = ['d8_2', 'd8_3', 'e584_1', 'g_id_1', 'g_id_2', 'g_id_3', 'g_id_1a', 'g_id_2a', 'g_id_3a', 'g126_1', 'g126_2', 'g126_3', 'g126_4', 'g126_5', 'g126_6', 'g126_7', 'g126_8', 'g127_3', 'g128_1', 'g129_2', 'g130_1', 'g131_1', 'g133_1', 'g133_2', 'g134_1', 'g134_2', 'g134_3', 'g134_4', 'g134_5', 'g134_6', 'g134_7', 'g134_8', 'g135_3', 'g136_1', 'g137_2', 'g138_1', 'g139_1', 'g141_1', 'g141_2', 'g143', 'g144_1', 'g144_2_1', 'g144_2_2', 'g144_2_3', 'g144_2_4', 'g144_2_5', 'g259', 'g148_1_1', 'g148_1_2', 'g148_1_3', 'g148_1_5', 'g148_1_6', 'g148_1_7', 'g148_1_8', 'g148_1_9', 'g148_1_10', 'g148_1_11', 'g148_1_12', 'g148_2_1', 'g148_2_2', 'g148_2_3', 'g148_2_5', 'g148_2_6', 'g148_2_7', 'g148_2_8', 'g148_2_9', 'g148_2_10', 'g148_2_11', 'g148_2_12', 'g148_3', 'g148_4', 'g148_5_1', 'g148_5_2', 'g257', 'g153_1', 'g153_2', 'g258_1', 'g154_1', 'h155_1', 'h156_1', 'h252_1', 'h160_1', 'h160_2', 'h163_1', 'h163_2', 'h164', 'h165', 'h166', 'h269_1', 'h167_1_3', 'h167_2_3', 'h167_3_3', 'h167_4_3', 'h170_3', 'h271_1', 'h171_1', 'h172_1', 'h173_1', 'i174', 'i175', 'eg_ps2', 'MTO_CUOTA', 'MTO_EMER', 'MTO_HOGCON', 'MTO_DESAY', 'MTO_ALMUE', 'MTO_VACAS', 'MTO_OVEJA', 'MTO_CABALL', 'INDACELIAC', 'INDAEMER', 'PT1', 'PT2', 'PT4', 'HT13', 'YHOG', 'AFAM_H_DEC', 'AFAM_H', 'TUS_H_DEC', 'TUS_H', 'lp_06', 'li_06', 'lp_17', 'li_17', 'monto_imput_UTE', 'monto_imput_GAS', 'monto_imput_OSE', 'H_FONASA', 'montoGAS_RRAA', 'montoUTE_RRAA', 'montoOSE_RRAA', 'W_TRI', 'W_SEM', 'W_ANO']\n",
    "\n",
    "# Variable objetivo\n",
    "target = 'YDA'\n",
    "ech['log_YDA'] = np.log1p(ech[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc298b5",
   "metadata": {},
   "source": [
    "No se incluyeron las siguientes variables porque si bien aparecen en el Excel no aparecen en el dataframe:\n",
    "\n",
    "Dicotomicas:\n",
    "- h2_cv: HOGAR CONTIN√öA RESIDIENDO EN LA MISMA VIVIENDA\n",
    "- c5_1, c5_3 a c5_9: PROBLEMAS DE LA VIVIENDA\n",
    "- e0_cv: NUEVO MIEMBRO DEL HOGAR\n",
    "- e01_cv: NUEVO MIEMBRO DEPENDE DEL FONDO DE ALIMENTACI√ìN\n",
    "- e1_cv: MIEMBRO CONTIN√öA RESIDIENDO EN EL HOGAR\n",
    "- f304: MANTIENE MISMO TRABAJO PRINCIPAL DECLARADO EN ENTREVISTA ANTERIOR\n",
    "- f302: REALIZA LAS MISMAS TAREAS\n",
    "- f303: EL ESTABLECIMIENTO CONTINUA REALIZANDO LAS MISMAS TAREAS\n",
    "- f311: MANTIENE MISMO TRABAJO SECUNDARIO DECLARADO EN ENTREVISTA ANTERIOR\n",
    "- f309: REALIZA LAS MISMAS TAREAS\n",
    "- f310: EL ESTABLECIMIENTO CONTINUA REALIZANDO LAS MISMAS TAREAS\n",
    "- INFORMAL: TRABAJADOR INFORMAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143030cb",
   "metadata": {},
   "source": [
    "Algunas variables toman valores nan luego de recodificar como variables dicotomicas. Vemos que valores toman.\n",
    "\n",
    "El problema es que en el dataset hay valores que no son ni 1 ni 2 en algunas variables dicot√≥micas y cualquier valor diferente de 1 o 2 (por ejemplo, NaN, 9, 3, o cualquier otro c√≥digo especial) se transforma autom√°ticamente en NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64674146",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_a_explorar = ['c5_2', 'c5_10', 'c5_11', 'c5_12', 'c5_13', 'c6', 'd8_4', 'd15', 'd21_1', 'd21_2', 'd21_3', 'd21_6', 'd21_4', 'd21_5', 'd21_20', 'd21_7', 'd21_10', 'd21_11', 'd21_12', 'd21_13', 'd21_14', 'd21_15', 'd21_15_1', 'd21_15_3', 'd21_15_5', 'd21_16', 'd21_16_1', 'd21_16_2', 'd21_21', 'd21_17', 'd21_18', 'd21_19', 'd181_unificado', 'd231', 'd184_unificado', 'h155', 'h156', 'h272', 'h273', 'h274', 'h252', 'h159', 'h160', 'h161', 'h162', 'h227', 'h269', 'h167_2', 'h167_3', 'h167_4', 'h169', 'h271', 'h171', 'h172', 'h173', 'i228', 'i259', 'eg_ps1', 'pobre06', 'indig06', 'pobre17', 'indig17', 'pobre_multi', 'e26', 'e29_1', 'e29_2', 'e29_3', 'e29_4', 'e29_5', 'e31', 'e32', 'e33', 'e185', 'e38', 'e45_4_1_cv', 'e45_cvb', 'e46_cv', 'e48', 'e581a', 'e582', 'e197_1', 'e201_1b', 'e201_1c', 'e201_1d', 'e51_6b', 'e215_1', 'e218_1', 'e221_1', 'e224_1', 'e559', 'e584', 'e59', 'f269', 'f270', 'f271', 'f272', 'f274', 'f276', 'f82', 'f84', 'f278', 'f280_1', 'f280_2', 'f280_3', 'f281_1', 'f281_2', 'f281_3', 'f281_4', 'f75', 'f81', 'f266_2', 'f267', 'f268', 'f289', 'f290', 'f291_a', 'f291_b', 'f292', 'f293', 'f294', 'f295', 'f96', 'f99', 'f100', 'f102', 'f298', 'f299', 'f114', 'f115', 'f300', 'f116', 'f117', 'f123', 'f124_1', 'f124_2', 'f124_3', 'f124_5', 'g250_1', 'g250_2', 'g250_5', 'g250_3', 'g250_4', 'g127', 'g128', 'g129', 'g129_1', 'g130', 'g131', 'g133', 'g_st_1', 'g251_1', 'g251_2', 'g251_5', 'g251_3', 'g251_4', 'g135', 'g136', 'g137', 'g137_1', 'g138', 'g139', 'g141', 'g_itnd_1', 'g_itnd_2', 'g144', 'g_it_1', 'g_it_2', 'g149', 'g149_1', 'g150', 'g255', 'g152', 'g153', 'g258', 'g154', 'SUBEMPLEO', 'indig06', 'pobre17', 'indig17']\n",
    "\n",
    "for col in variables_a_explorar:\n",
    "    if col in ech.columns:\n",
    "        print(f\"\\nValores √∫nicos en '{col}':\")\n",
    "        print(ech[col].unique())\n",
    "    else:\n",
    "        print(f\"\\nLa columna '{col}' no est√° en el DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d27e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-codificaci√≥n de variables dicot√≥micas (1 ‚Üí 1, 2 ‚Üí 0, 0 ‚Üí 0, NaN ‚Üí 0)\n",
    "for col in dicotomicas:\n",
    "    if col in ech.columns:\n",
    "        ech[col] = ech[col].map({1: 1, 2: 0, 0: 0, np.nan: 0}).fillna(0)\n",
    "    else:\n",
    "        print(f\"Advertencia: {col} no est√° en el DataFrame y se omite.\")\n",
    "\n",
    "# Verificacion: Deber√≠amos ver solo [0, 1] o [1] (si la variable es constante).\n",
    "for col in dicotomicas:\n",
    "    print(f\"{col}: {ech[col].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fc6d46",
   "metadata": {},
   "source": [
    "No se incluyeron las siguientes variables porque si bien aparecen en el Excel no aparecen en el dataframe:\n",
    "\n",
    "Nominal:\n",
    "- h4_1_cv: DEPARTAMENTO DEL NUEVO DOMICILIO\n",
    "- SIT_OCUP: SITUACI√ìN EN LA OCUPACI√ìN\n",
    "- SECTOR_F: TIPO DE SECTOR\n",
    "\n",
    "Ordinales: NIV_EDU: NIVEL EDUCATIVO\n",
    "\n",
    "Discretas: ronda: ?\n",
    "\n",
    "Continuas: W: PONDERADOR MENSUAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6214b897",
   "metadata": {},
   "source": [
    "El preprocesamiento original no aplicaba imputacion para valores faltantes por lo que al correr los modelos daba error.\n",
    "\n",
    "Comentamos el preprocesamiento original y corremos un preprocesamiento mas robusto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8f4db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== PASO 2: Preprocesamiento ====== \n",
    "\n",
    "# Codificaci√≥n de ordinales ‚Äì si los valores est√°n bien ordenados\n",
    "#ordinal_encoder = OrdinalEncoder()\n",
    "\n",
    "# Escalador para continuas\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# Codificador para nominales\n",
    "#onehot = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# ColumnTransformer para aplicar todo junto\n",
    "#preprocessor = ColumnTransformer(transformers=[\n",
    "#    ('nom', onehot, nominales),\n",
    "#    ('ord', ordinal_encoder, ordinales),\n",
    "#    ('scale', scaler, numericas_continuas)\n",
    "#], remainder='passthrough')  # deja discretas y dicot√≥micas sin tocar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba7b606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a167e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelines individuales con imputaci√≥n\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "nominal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "binary_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))  # No codifica, ya vienen como 0/1\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColumnTransformer final\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numericas_continuas),\n",
    "    ('ord', ordinal_transformer, ordinales),\n",
    "    ('nom', nominal_transformer, nominales),\n",
    "    ('bin', binary_transformer, dicotomicas),\n",
    "    ('disc', numeric_transformer, numericas_discretas)  # puede usar el mismo pipeline que num√©ricas continuas\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68d4f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== PASO 3: Separar X e y ======\n",
    "\n",
    "#X = ech[dicotomicas + nominales + ordinales + numericas_discretas + numericas_continuas]\n",
    "\n",
    "# Eliminar variables con fuga de informaci√≥n\n",
    "#vars_a_eliminar = ['YDA', 'YDA_SVL']  # ajustar seg√∫n auditor√≠a\n",
    "#X = X.drop(columns=[col for col in vars_a_eliminar if col in X.columns])\n",
    "\n",
    "#y = ech['log_YDA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7210234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Separar features y target\n",
    "X = ech[dicotomicas + nominales + ordinales + numericas_discretas + numericas_continuas]\n",
    "y = ech['log_YDA']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166ac08",
   "metadata": {},
   "source": [
    "Detectamos que los modelos que corrimos mas abajo sobreajustaban. Esto ocurria porque teniamos la variable num__YDA (generada en el pipeline) que aporta el 99.8% de la importancia del modelo segun un feature importance. Por lo tanto eliminamos las variables sospechosas antes del split y del fit del preprocesador y nos aseguramos de que:\n",
    "- Nunca se usen estas variables en el entrenamiento.\n",
    "- El pipeline y las m√©tricas se calculen solo con variables leg√≠timas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91970948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Filtrar listas de variables para que no incluyan columnas eliminadas\n",
    "#dicotomicas = [c for c in dicotomicas if c not in vars_a_eliminar]\n",
    "#nominales = [c for c in nominales if c not in vars_a_eliminar]\n",
    "#ordinales = [c for c in ordinales if c not in vars_a_eliminar]\n",
    "#numericas_discretas = [c for c in numericas_discretas if c not in vars_a_eliminar]\n",
    "#numericas_continuas = [c for c in numericas_continuas if c not in vars_a_eliminar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7640bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Train-test split UNA sola vez\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef17c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver columnas duplicadas en X_train\n",
    "from collections import Counter\n",
    "\n",
    "contador_columnas = Counter(X_train.columns)\n",
    "columnas_duplicadas = [col for col, count in contador_columnas.items() if count > 1]\n",
    "\n",
    "print(\"Columnas duplicadas:\", columnas_duplicadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5026fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver variables en comun entre los conjuntos\n",
    "sets = {\n",
    "    \"ordinales\": set(ordinales),\n",
    "    \"nominales\": set(nominales),\n",
    "    \"numericas_discretas\": set(numericas_discretas),\n",
    "    \"numericas_continuas\": set(numericas_continuas),\n",
    "    \"dicotomicas\": set(dicotomicas)\n",
    "}\n",
    "\n",
    "for nombre1, set1 in sets.items():\n",
    "    for nombre2, set2 in sets.items():\n",
    "        if nombre1 != nombre2:\n",
    "            interseccion = set1 & set2\n",
    "            if interseccion:\n",
    "                print(f\"Variables en com√∫n entre {nombre1} y {nombre2}: {interseccion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08268d62",
   "metadata": {},
   "source": [
    "El pipeline final dio error porque algunas variables estaban repetidas porque se trajeron del Excel que las repetia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4808ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== PASO 5: Pipeline final ======\n",
    "\n",
    "#pipeline = Pipeline(steps=[\n",
    "#    ('preprocesamiento', preprocessor)\n",
    "#])\n",
    "\n",
    "# Fit-transform solo para ver el shape de salida\n",
    "#X_train_proc = pipeline.fit_transform(X_train)\n",
    "#X_test_proc = pipeline.transform(X_test)\n",
    "#print(\"Shape del dataset procesado:\", X_train_proc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61645173",
   "metadata": {},
   "source": [
    "Dataset original (ech) con 55.923 filas, entonces con un test_size=0.2:\n",
    "- X_train y y_train ‚Üí 80% de los datos ‚âà 44.738 filas\n",
    "- X_test y y_test ‚Üí 20% de los datos ‚âà 11.185 filas\n",
    "\n",
    "Coincide con Shape del dataset procesado: (44738, 2889):\n",
    "- Las 44.738 filas son del conjunto de entrenamiento.\n",
    "- Las 2.889 columnas son el resultado del preprocesamiento, principalmente por el OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0577d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de que no est√©n en ninguna lista usada en preprocessor\n",
    "for lista in [numericas_continuas, numericas_discretas, ordinales, nominales, dicotomicas]:\n",
    "    if 'YDA' in lista:\n",
    "        lista.remove('YDA')\n",
    "    if 'YDA_SVL' in lista:\n",
    "        lista.remove('YDA_SVL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c919a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Fit-transform SOLO con train y transform test\n",
    "X_train_proc = preprocessor.fit_transform(X_train)\n",
    "X_test_proc  = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Shapes tras preprocesamiento:\")\n",
    "print(\"X_train_proc:\", X_train_proc.shape)\n",
    "print(\"X_test_proc :\", X_test_proc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6407a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar cu√°ntas filas tienen al menos un NaN\n",
    "n_filas_nan = np.isnan(X_train_proc).sum(axis=1)\n",
    "n_filas_con_nan = np.sum(n_filas_nan > 0)\n",
    "\n",
    "print(f\"N√∫mero de filas con al menos un NaN: {n_filas_con_nan}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8f07b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar columnas con NaN\n",
    "n_columnas_con_nan = np.sum(np.isnan(X_train_proc).any(axis=0))\n",
    "print(f\"N√∫mero de columnas con al menos un NaN: {n_columnas_con_nan}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbda847",
   "metadata": {},
   "source": [
    "Eliminamos redundancias autom√°ticas (varianza cero y correlaci√≥n perfecta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3398ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a DataFrame con nombres de columnas\n",
    "X_train_proc = pd.DataFrame(\n",
    "    X_train_proc,\n",
    "    columns=preprocessor.get_feature_names_out()\n",
    ")\n",
    "X_test_proc = pd.DataFrame(\n",
    "    X_test_proc,\n",
    "    columns=preprocessor.get_feature_names_out()\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Shape inicial tras preprocesamiento: {X_train_proc.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8243243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2Ô∏è‚É£ Funciones de limpieza\n",
    "# ============================================\n",
    "def eliminar_varianza_cero(df):\n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "    selector = VarianceThreshold(threshold=0.0)\n",
    "    selector.fit(df)\n",
    "    cols_quitar = df.columns[~selector.get_support()].tolist()\n",
    "    df_filtrado = df.drop(columns=cols_quitar)\n",
    "    return df_filtrado, cols_quitar\n",
    "\n",
    "def eliminar_correlacion_perfecta(df):\n",
    "    corr_matrix = df.corr()\n",
    "    cols_quitar = set()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if corr_matrix.iloc[i, j] == 1:\n",
    "                colname = corr_matrix.columns[i]\n",
    "                cols_quitar.add(colname)\n",
    "    df_filtrado = df.drop(columns=list(cols_quitar))\n",
    "    return df_filtrado, list(cols_quitar)\n",
    "\n",
    "def eliminar_correlacion_alta(df, umbral=0.95):\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    cols_quitar = [column for column in upper.columns if any(upper[column] > umbral)]\n",
    "    df_filtrado = df.drop(columns=cols_quitar)\n",
    "    return df_filtrado, cols_quitar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29df448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3Ô∏è‚É£ Aplicar limpieza a TRAIN y TEST\n",
    "# ============================================\n",
    "# Varianza cero\n",
    "X_train_proc, cols_var0 = eliminar_varianza_cero(X_train_proc)\n",
    "X_test_proc = X_test_proc.drop(columns=cols_var0, errors='ignore')\n",
    "\n",
    "# Correlaci√≥n perfecta\n",
    "X_train_proc, cols_corr1 = eliminar_correlacion_perfecta(X_train_proc)\n",
    "X_test_proc = X_test_proc.drop(columns=cols_corr1, errors='ignore')\n",
    "\n",
    "# Correlaci√≥n alta\n",
    "X_train_proc, cols_corr_high = eliminar_correlacion_alta(X_train_proc, umbral=0.95)\n",
    "X_test_proc = X_test_proc.drop(columns=cols_corr_high, errors='ignore')\n",
    "\n",
    "print(f\"üìâ Shape final tras limpieza: {X_train_proc.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61be7a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4Ô∏è‚É£ Guardar columnas eliminadas para trazabilidad\n",
    "# ============================================\n",
    "df_eliminadas = pd.DataFrame({\n",
    "    \"varianza_cero\": pd.Series(cols_var0),\n",
    "    \"corr_perfecta\": pd.Series(cols_corr1),\n",
    "    \"corr_alta\": pd.Series(cols_corr_high)\n",
    "})\n",
    "df_eliminadas.to_excel(\"columnas_eliminadas.xlsx\", index=False)\n",
    "print(\"üìÇ Archivo 'columnas_eliminadas.xlsx' guardado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab0c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasta aca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409ab21d",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f734ff",
   "metadata": {},
   "source": [
    "Comentamos los modelos iniciales con MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23f45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importo librerias\n",
    "#from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "#from sklearn.tree import DecisionTreeRegressor\n",
    "#from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.metrics import make_scorer, mean_squared_error\n",
    "#import numpy as np\n",
    "#import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a56b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar XGBoost y LightGBM si est√°n disponibles\n",
    "#try:\n",
    "#    from xgboost import XGBRegressor\n",
    "#except ImportError:\n",
    "#    XGBRegressor = None\n",
    "\n",
    "#try:\n",
    "#    from lightgbm import LGBMRegressor\n",
    "#except ImportError:\n",
    "#    LGBMRegressor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09149766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelos a evaluar\n",
    "#modelos = {\n",
    "#    \"LinearRegression\": LinearRegression(),\n",
    "#    \"Ridge\": Ridge(),\n",
    "#    \"Lasso\": Lasso(),\n",
    "#    \"DecisionTree\": DecisionTreeRegressor(),\n",
    "#    \"RandomForest\": RandomForestRegressor(n_jobs=-1),\n",
    "#    \"GradientBoosting\": GradientBoostingRegressor()\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6048b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar XGBoost si est√° disponible\n",
    "#if XGBRegressor:\n",
    "#    modelos[\"XGBoost\"] = XGBRegressor(n_jobs=-1)\n",
    "\n",
    "# Agregar LightGBM si est√° disponible\n",
    "#if LGBMRegressor:\n",
    "#    modelos[\"LightGBM\"] = LGBMRegressor(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"¬øHay NaN en X_train_proc?\", np.isnan(X_train_proc).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c55612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©trica: MSE negativo (lo convierte a positivo m√°s abajo)\n",
    "#scoring = make_scorer(mean_squared_error, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14a829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecuta validaci√≥n cruzada y muestra resultados\n",
    "#resultados = {}\n",
    "#for nombre, modelo in modelos.items():\n",
    "#    scores = cross_val_score(modelo, X_train_proc, y_train, cv=5, scoring=scoring)\n",
    "#    mse_promedio = -np.mean(scores)\n",
    "#    print(f\"{nombre}: MSE promedio = {mse_promedio:,.2f}\")\n",
    "#    resultados[nombre] = mse_promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac79b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar resultados ordenados\n",
    "#df_resultados = pd.DataFrame(resultados).T.sort_values(by='RMSE promedio')\n",
    "#print(\"\\nComparaci√≥n de modelos por RMSE (menor es mejor):\")\n",
    "#display(df_resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228e52cc",
   "metadata": {},
   "source": [
    "Entrenar modelos usando X_train_proc / y_train y evaluar en X_test_proc / y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d4f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Importo librerias\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96baa2d",
   "metadata": {},
   "source": [
    "Primera funcion para evaluar modelo (la comentamos para no correrla)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df81f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilidad para evaluar en ambas escalas\n",
    "#def evaluar_modelo(modelo, Xtr, ytr, Xte, yte, nombre=\"Modelo\"):\n",
    "    # Entrenar\n",
    "#    modelo.fit(Xtr, ytr)\n",
    "    # Predecir\n",
    "#    y_pred = modelo.predict(Xte)\n",
    "\n",
    "    # M√©tricas en escala log (si y es log_YDA)\n",
    "#    mae = mean_absolute_error(yte, y_pred)\n",
    "#    mse = mean_squared_error(yte, y_pred)\n",
    "#    rmse = np.sqrt(mse)\n",
    "#    r2 = r2_score(yte, y_pred)\n",
    "\n",
    "#    print(f\"\\n=== {nombre} | M√©tricas en escala log (log_YDA) ===\")\n",
    "#    print(f\"MAE : {mae:,.4f}\")\n",
    "#    print(f\"MSE : {mse:,.4f}\")\n",
    "#    print(f\"RMSE: {rmse:,.4f}\")\n",
    "#    print(f\"R¬≤  : {r2:,.4f}\")\n",
    "\n",
    "    # M√©tricas en escala original (deslogarizando)\n",
    "#    yte_orig = np.expm1(yte)\n",
    "#    y_pred_orig = np.expm1(y_pred)\n",
    "\n",
    "#    mae_o = mean_absolute_error(yte_orig, y_pred_orig)\n",
    "#    mse_o = mean_squared_error(yte_orig, y_pred_orig)\n",
    "#    rmse_o = np.sqrt(mse_o)\n",
    "#    r2_o = r2_score(yte_orig, y_pred_orig)\n",
    "\n",
    "#    print(f\"\\n=== {nombre} | M√©tricas en escala original (pesos) ===\")\n",
    "#    print(f\"MAE : {mae_o:,.2f}\")\n",
    "#    print(f\"MSE : {mse_o:,.2f}\")\n",
    "#    print(f\"RMSE: {rmse_o:,.2f}\")\n",
    "#    print(f\"R¬≤  : {r2_o:,.4f}\")\n",
    "\n",
    "#    return {\n",
    "#        \"mae_log\": mae, \"mse_log\": mse, \"rmse_log\": rmse, \"r2_log\": r2,\n",
    "#        \"mae\": mae_o, \"mse\": mse_o, \"rmse\": rmse_o, \"r2\": r2_o,\n",
    "#        \"y_pred\": y_pred, \"y_pred_orig\": y_pred_orig\n",
    "#    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e2c15e",
   "metadata": {},
   "source": [
    "Funcion correjida con validacion defensiva (porque la regresion lineal daba error con la funcion anterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97793fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluar_modelo(modelo, X_train, y_train, X_test, y_test, nombre=\"modelo\"):\n",
    "    print(\"üü¢ Entrenando modelo...\")\n",
    "    modelo.fit(X_train, y_train)\n",
    "\n",
    "    print(\"üü¢ Generando predicciones...\")\n",
    "    y_pred_train = modelo.predict(X_train)\n",
    "    y_pred_test = modelo.predict(X_test)\n",
    "\n",
    "    print(\"üîç Verificando predicciones...\")\n",
    "    print(f\"M√°ximo y_pred_test: {np.max(y_pred_test)}\")\n",
    "    print(f\"M√≠nimo y_pred_test: {np.min(y_pred_test)}\")\n",
    "\n",
    "    print(\"üìä Calculando m√©tricas en escala log...\")\n",
    "    r2 = r2_score(y_test, y_pred_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "    # üîÅ Revertimos a escala de pesos, evitando overflow en np.exp\n",
    "    y_test_clip = np.clip(y_test, 0, 30)\n",
    "    y_pred_clip = np.clip(y_pred_test, 0, 30)\n",
    "\n",
    "    y_test_original = np.exp(y_test_clip)\n",
    "    y_pred_original = np.exp(y_pred_clip)\n",
    "\n",
    "    print(f\"M√°ximo y_pred_original (clipped): {np.max(y_pred_original)}\")\n",
    "\n",
    "    print(\"üìä Calculando m√©tricas en pesos...\")\n",
    "    mae_pesos = mean_absolute_error(y_test_original, y_pred_original)\n",
    "    rmse_pesos = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n",
    "\n",
    "    print(\"‚úÖ M√©tricas finales:\")\n",
    "    print(f\"Modelo: {nombre}\")\n",
    "    print(f\"R2: {r2:.4f} | MAE_log: {mae:.2f} | RMSE_log: {rmse:.2f}\")\n",
    "    print(f\"MAE_pesos: {mae_pesos:.2f} | RMSE_pesos: {rmse_pesos:.2f}\")\n",
    "\n",
    "    return {\n",
    "        \"modelo\": nombre,\n",
    "        \"R2\": r2,\n",
    "        \"MAE_log\": mae,\n",
    "        \"RMSE_log\": rmse,\n",
    "        \"MAE_pesos\": mae_pesos,\n",
    "        \"RMSE_pesos\": rmse_pesos\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f653215e",
   "metadata": {},
   "source": [
    "La funcion original daba error porque etabamos usando np.exp(y_pred_test) para volver a la escala original (en pesos), pero las predicciones incluyen valores extremadamente grandes. Eso explota al aplicar np.exp() y da como resultado inf, lo que rompe las m√©tricas como mean_squared_error. Entonces limitamos las predicciones antes de aplicar np.exp() con np.clip(). Esto evitar√° que np.exp() explote al exponenciar n√∫meros muy altos.\n",
    "\n",
    "Esta estrategia es valida cuando:\n",
    "- Est√°s evaluando predicciones en log-transformed regression.\n",
    "- El modelo produce algunos valores desproporcionados (outliers extremos).\n",
    "- El objetivo es evitar que un exp() distorsione totalmente las m√©tricas.\n",
    "\n",
    "Adem√°s, ya ten√©s tu variable objetivo (log_YDA) log-transformada, y no est√°s usando estos valores para reentrenamiento, sino para evaluaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88906569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Split en crudo (sin preprocesar a√∫n)\n",
    "\n",
    "# X = ech[dicotomicas + nominales + ordinales + numericas_discretas + numericas_continuas]\n",
    "# y = ech['log_YDA']   # ya definido previamente\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fea674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Ajustar SOLO con TRAIN y transformar TRAIN y TEST:\n",
    "# El preprocessor es el ColumnTransformer definido antes.\n",
    "\n",
    "#preprocessor.fit(X_train)\n",
    "\n",
    "#X_train_proc = preprocessor.transform(X_train)\n",
    "#X_test_proc  = preprocessor.transform(X_test)\n",
    "\n",
    "#print(\"Shapes tras preprocesamiento:\")\n",
    "#print(\"X_train_proc:\", X_train_proc.shape)\n",
    "#print(\"X_test_proc :\", X_test_proc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d9aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos valores invalidos\n",
    "def verificar_valores_invalidos(X, nombre):\n",
    "    print(f\"üîé Verificando {nombre}:\")\n",
    "    print(\"NaN:\", np.isnan(X).sum())\n",
    "    print(\"Inf:\", np.isinf(X).sum())\n",
    "    print(\"Valores demasiado grandes:\", np.sum(np.abs(X) > 1e10))\n",
    "\n",
    "verificar_valores_invalidos(X_train_proc, \"X_train_proc\")\n",
    "verificar_valores_invalidos(X_test_proc, \"X_test_proc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c7c904",
   "metadata": {},
   "source": [
    "No hay problemas en los conjuntos de X. Vemos y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c611bb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîé y_train\")\n",
    "print(\"NaN:\", np.isnan(y_train).sum())\n",
    "print(\"Inf:\", np.isinf(y_train).sum())\n",
    "print(\"Valores extremos:\", np.sum(np.abs(y_train) > 1e10))\n",
    "\n",
    "print(\"\\nüîé y_test\")\n",
    "print(\"NaN:\", np.isnan(y_test).sum())\n",
    "print(\"Inf:\", np.isinf(y_test).sum())\n",
    "print(\"Valores extremos:\", np.sum(np.abs(y_test) > 1e10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c47f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y_train - min:\", np.min(y_train), \"max:\", np.max(y_train))\n",
    "print(\"y_test - min:\", np.min(y_test), \"max:\", np.max(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6debf5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "varianzas = np.var(X_train_proc, axis=0)\n",
    "print(\"Columnas con varianza cero:\", np.sum(varianzas == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976dcfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valor m√°ximo absoluto en X_train_proc:\", np.max(np.abs(X_train_proc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbcf85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_lr = LinearRegression()\n",
    "\n",
    "try:\n",
    "    modelo_lr.fit(X_train_proc, y_train)\n",
    "    print(\"‚úÖ Fit correcto\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error durante el fit:\", e)\n",
    "\n",
    "try:\n",
    "    y_pred = modelo_lr.predict(X_test_proc)\n",
    "    print(\"‚úÖ Predict correcto\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error durante el predict:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee3e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Modelo 1: Regresi√≥n Lineal\n",
    "\n",
    "resultados_modelos = []\n",
    "\n",
    "modelo_lr = LinearRegression()\n",
    "res_lr = evaluar_modelo(modelo_lr, X_train_proc, y_train, X_test_proc, y_test, nombre=\"Linear Regression\")\n",
    "\n",
    "# Verificaci√≥n defensiva antes de agregar\n",
    "if all(np.isfinite(v) for k, v in res_lr.items() if k != \"modelo\"):\n",
    "    resultados_modelos.append(res_lr)\n",
    "else:\n",
    "    print(\"‚ùå Resultado no agregado por valores inv√°lidos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdceeb1",
   "metadata": {},
   "source": [
    "Esos resultados me confirman que la regresi√≥n lineal sigue completamente inestable, incluso despu√©s de las modificaciones.\n",
    "- R¬≤ negativo y enorme: Un R¬≤ muy negativo indica que el modelo no solo no est√° explicando nada, sino que lo est√° haciendo peor que una media constante. En un modelo estable y bien especificado, aunque sea malo, deber√≠amos ver R¬≤ cercano a 0, no un valor tan extremo.\n",
    "- Predicciones extremas: M√°ximos y m√≠nimos de y_pred_test muy alejados de lo esperado (miles de millones en log transformado) son s√≠ntoma de multicolinealidad o coeficientes enormes. Esto pasa porque la regresi√≥n lineal est√°ndar no tiene regularizaci√≥n, y si hay correlaci√≥n fuerte entre variables o un n√∫mero de features muy grande, los coeficientes explotan.\n",
    "- MAE y RMSE gigantes en pesos: M√°s de 50 mil millones de pesos de MAE y 735 mil millones de RMSE indican que el modelo no es usable en producci√≥n. Esto lo vimos antes: el comportamiento t√≠pico de un modelo que est√° sobreajustado a ruido y no generaliza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c5c02",
   "metadata": {},
   "source": [
    "Hacemos un VIF (Variance Inflation Factor) de las variables procesadas para detectar multicolinealidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0102f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5e2574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 1) Calcular VIF en datos procesados\n",
    "# =========================================\n",
    "\n",
    "# X_train_proc es un array numpy, as√≠ que lo pasamos a DataFrame\n",
    "# Usamos las columnas generadas por el preprocessor\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "X_train_df = pd.DataFrame(X_train_proc, columns=feature_names)\n",
    "\n",
    "# Calculamos VIF\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_train_df.columns\n",
    "vif_data[\"VIF\"] = [\n",
    "    variance_inflation_factor(X_train_df.values, i) for i in range(X_train_df.shape[1])\n",
    "]\n",
    "\n",
    "# Ordenamos por VIF descendente\n",
    "vif_data = vif_data.sort_values(by=\"VIF\", ascending=False)\n",
    "print(\"\\nüìä Top 20 features con mayor VIF:\")\n",
    "print(vif_data.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174ce8d8",
   "metadata": {},
   "source": [
    "üìä Top 20 features con mayor VIF:\n",
    "              feature  VIF\n",
    "1442  nom__f72_2_8413  inf\n",
    "1409  nom__f72_2_7210  inf\n",
    "1423  nom__f72_2_7830  inf\n",
    "1422  nom__f72_2_7820  inf\n",
    "1421  nom__f72_2_7810  inf\n",
    "1420  nom__f72_2_7730  inf\n",
    "1419  nom__f72_2_7729  inf\n",
    "1418  nom__f72_2_7721  inf\n",
    "1417  nom__f72_2_7710  inf\n",
    "1416  nom__f72_2_7500  inf\n",
    "1415  nom__f72_2_7490  inf\n",
    "1414  nom__f72_2_7420  inf\n",
    "1413  nom__f72_2_7410  inf\n",
    "1412  nom__f72_2_7320  inf\n",
    "1411  nom__f72_2_7310  inf\n",
    "1410  nom__f72_2_7220  inf\n",
    "1408  nom__f72_2_7120  inf\n",
    "1590      nom__f305_2  inf\n",
    "1407  nom__f72_2_7110  inf\n",
    "1406  nom__f72_2_7020  inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8b4838",
   "metadata": {},
   "source": [
    "Esos resultados son una se√±al clar√≠sima de que tienes multicolinealidad perfecta en muchas variables, y adem√°s concentrada en un bloque muy espec√≠fico: las columnas nom__f72_2_XXXX y alguna como nom__f305_2.\n",
    "\n",
    "Que el VIF sea inf significa que alguna de esas variables es combinaci√≥n lineal exacta de otras, lo cual es t√≠pico cuando:\n",
    "\n",
    "Hay One-Hot Encoding con categor√≠as que nunca se presentan en train o tienen representaci√≥n redundante.\n",
    "\n",
    "Se incluyen todas las dummies de una variable categ√≥rica (no se hace drop='first').\n",
    "\n",
    "Hay columnas que son copias exactas o que sumadas a otras siempre dan lo mismo (dummy trap)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979a886",
   "metadata": {},
   "source": [
    "üí° Conclusiones pr√°cticas:\n",
    "\n",
    "Tu OneHotEncoder probablemente est√° generando columnas completamente redundantes en algunas categor√≠as (las de f72_2 parecen ser un caso de codificaci√≥n con much√≠simas categor√≠as).\n",
    "\n",
    "Esto explica por qu√© Linear Regression explotaba** ‚Üí la matriz X'X es singular o casi singular.\n",
    "\n",
    "Ridge puede estabilizar algo, pero si el VIF es inf es porque tienes redundancia total; lo ideal ser√≠a reducir esas columnas.\n",
    "\n",
    "Dado que ya usas drop='first' en el OneHotEncoder, podr√≠a estar pasando que algunas categor√≠as est√°n 100% correlacionadas con otras variables (por ejemplo, dummies para ubicaciones que coinciden con otra variable geogr√°fica).`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa61db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# 2) Entrenar Ridge Regression\n",
    "# =========================================\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "modelo_ridge = Ridge(alpha=10)  # alpha controla regularizaci√≥n; probar 1, 10, 100\n",
    "modelo_ridge.fit(X_train_proc, y_train)\n",
    "\n",
    "y_pred = modelo_ridge.predict(X_test_proc)\n",
    "\n",
    "# Evaluaci√≥n\n",
    "mae_log = mean_absolute_error(y_test, y_pred)\n",
    "mse_log = mean_squared_error(y_test, y_pred)\n",
    "rmse_log = np.sqrt(mse_log)\n",
    "r2_log = r2_score(y_test, y_pred)\n",
    "\n",
    "# Invertimos escala log para m√©tricas en pesos\n",
    "y_pred_original = np.exp(y_pred)\n",
    "y_test_original = np.exp(y_test)\n",
    "\n",
    "mae_pesos = mean_absolute_error(y_test_original, y_pred_original)\n",
    "rmse_pesos = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n",
    "\n",
    "print(\"\\n‚úÖ M√©tricas Ridge Regression:\")\n",
    "print(f\"R2: {r2_log:.4f} | MAE_log: {mae_log:.4f} | RMSE_log: {rmse_log:.4f}\")\n",
    "print(f\"MAE_pesos: {mae_pesos:.2f} | RMSE_pesos: {rmse_pesos:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8202b214",
   "metadata": {},
   "source": [
    "R¬≤ = 0.8284: \n",
    "- Esto significa que el modelo est√° explicando m√°s del 82% de la variabilidad en la escala logar√≠tmica, lo cual es muy bueno para un dataset socioecon√≥mico con alta dispersi√≥n.\n",
    "- A diferencia de la regresi√≥n lineal simple (que estaba totalmente inestable), el Ridge est√° controlando la multicolinealidad y manteniendo los coeficientes en rangos razonables.\n",
    "\n",
    "Errores bajos en log:\n",
    "- Un MAE_log = 0.2006 y un RMSE_log = 0.2770 son bastante competitivos.\n",
    "- En t√©rminos pr√°cticos, el error relativo promedio es de aproximadamente un 20% en escala log, lo cual es aceptable.\n",
    "\n",
    "Errores en pesos realistas:\n",
    "- MAE_pesos ‚âà 56 mil pesos y RMSE_pesos ‚âà 3.4 M indican que, aunque hay outliers que generan error grande en RMSE, la predicci√≥n media est√° razonablemente cerca de los valores reales.\n",
    "- Este tipo de error es much√≠simo menor que el que vimos en la lineal cl√°sica.\n",
    "\n",
    "Estabilidad:\n",
    "- No se observan predicciones absurdas como en la lineal est√°ndar.\n",
    "- Esto significa que la regularizaci√≥n est√° funcionando y el modelo no est√° explotando con valores fuera de rango."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901dc9c2",
   "metadata": {},
   "source": [
    "üí° Interpretaci√≥n\n",
    "La regularizaci√≥n de Ridge est√° funcionando:\n",
    "Penaliza los coeficientes grandes, estabiliza el sistema y permite que la regresi√≥n se ajuste razonablemente pese a la alta correlaci√≥n entre features.\n",
    "\n",
    "Persisten outliers importantes:\n",
    "El salto grande de MAE a RMSE en pesos sugiere que unas pocas observaciones extremas est√°n penalizando bastante el RMSE.\n",
    "\n",
    "El VIF infinito ya no destroza el modelo gracias a la regularizaci√≥n, pero eliminar redundancias a√∫n podr√≠a mejorar estabilidad y reducir dimensionalidad.\n",
    "\n",
    "Ridge es una opci√≥n s√≥lida como baseline lineal antes de pasar a modelos m√°s complejos como Random Forest o Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b03a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Modelo 2: Random Forest\n",
    "# (hiperpar√°metros base; ajustamos luego)\n",
    "\n",
    "modelo_rf = RandomForestRegressor(\n",
    "    n_estimators=400,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "res_rf = evaluar_modelo(modelo_rf, X_train_proc, y_train, X_test_proc, y_test, nombre=\"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30138570",
   "metadata": {},
   "source": [
    "Interpretaci√≥n en escala log:\n",
    "- R¬≤ = 0.9398 ‚Üí Explicamos ~94% de la varianza de los ingresos log-transformados en el set de test. Esto es muy alto y superior al Ridge (0.8284), lo que indica que el modelo captura mejor las relaciones no lineales y posibles interacciones.\n",
    "- MAE_log = 0.09 y RMSE_log = 0.16 ‚Üí Errores peque√±os en escala log, lo que implica buena capacidad predictiva incluso en datos no vistos.\n",
    "\n",
    "Interpretaci√≥n en pesos:\n",
    "- MAE_pesos ‚âà 11.4 mil pesos ‚Üí Error medio absoluto relativamente bajo considerando que estamos modelando ingresos que pueden ir desde cero hasta m√°s de un mill√≥n de pesos.\n",
    "- RMSE_pesos ‚âà 33.6 mil pesos ‚Üí El error cuadr√°tico medio tambi√©n es bajo, aunque mayor que el MAE, lo que indica que a√∫n hay algunos errores grandes en casos puntuales (probablemente en los ingresos m√°s altos).\n",
    "\n",
    "Conclusi√≥n:\n",
    "- Muy prometedor para seguir con XGBoost y LightGBM.\n",
    "- Este modelo ya es √∫til para predicci√≥n pr√°ctica, pero podemos intentar afinarlo con RandomizedSearchCV para mejorar a√∫n m√°s.\n",
    "- Posible riesgo: podr√≠a haber algo de sobreajuste si la validaci√≥n no es lo suficientemente estricta (RF tiende a eso con muchos √°rboles y profundidad alta)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86dd5bf",
   "metadata": {},
   "source": [
    "Auditor√≠a de features (Feature Importance)\n",
    "\n",
    "Esto te permite ver cu√°les son las variables m√°s influyentes seg√∫n el modelo. Si aparece una variable que contiene informaci√≥n directa del ingreso (por ejemplo, otro tipo de ingreso o una suma de ingresos del hogar), entonces es probable que haya fuga de informaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fe1a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Obtener nombres de las columnas procesadas (despu√©s del pipeline)\n",
    "# Esto asume que hiciste fit en el pipeline con X_train\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "importancias = modelo_rf.feature_importances_\n",
    "\n",
    "# Crear DataFrame con los resultados\n",
    "importancias_df = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': importancias\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Mostrar las 20 m√°s importantes\n",
    "print(importancias_df.head(20))\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(importancias_df.head(20)['feature'][::-1], importancias_df.head(20)['importance'][::-1])\n",
    "plt.xlabel(\"Importancia\")\n",
    "plt.title(\"Top 20 variables m√°s importantes - Random Forest\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9e1db3",
   "metadata": {},
   "source": [
    "Observamos:\n",
    "- La variable num__YDA aporta el 99.8% de la importancia del modelo.\n",
    "- El resto de las variables tienen importancias casi nulas (valores del orden de 0.001 o incluso 0.000002).\n",
    "- Entre las m√°s importantes figuran tambi√©n HT11, YDA_SVL y YSVL, todas ellas potencialmente relacionadas con ingreso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2651c8",
   "metadata": {},
   "source": [
    "Interpretaci√≥n:\n",
    "\n",
    "- Esto es una se√±al clara de fuga de informaci√≥n.\n",
    "- YDA es tu variable objetivo (aunque aqu√≠ en su versi√≥n logar√≠tmica), y si aparece como predictor, entonces el modelo est√° simplemente \"viendo\" la respuesta, lo cual hace in√∫tiles las m√©tricas obtenidas.\n",
    "- Variables como YDA_SVL o YSVL tambi√©n suenan sospechosamente similares, por lo que es probable que sean funciones directas o indirectas del ingreso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca24ba8",
   "metadata": {},
   "source": [
    "Conclusi√≥n de esta parte:\n",
    "\n",
    "- Tu modelo est√° dominado por variables que contienen directamente (o de forma colineal) la variable que intent√°s predecir. \n",
    "- Esto invalida los resultados. Se debe excluir YDA y toda variable derivada del conjunto de predictores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6f72e",
   "metadata": {},
   "source": [
    "Random Forest con Validaci√≥n cruzada (Cross-Validation)\n",
    "\n",
    "Esto te dice si el rendimiento es estable entre distintas particiones de los datos. Si hay sobreajuste, el resultado promedio ser√° mucho m√°s bajo que el obtenido en tu partici√≥n test anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b44c119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Pipeline que combina preprocesamiento y modelo\n",
    "modelo_rf_cv = Pipeline(steps=[\n",
    "    ('preprocesamiento', preprocessor),\n",
    "    ('modelo', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Ejecutar CV con el pipeline\n",
    "scores = cross_val_score(modelo_rf_cv, X, y, cv=5, scoring='r2')\n",
    "\n",
    "print(\"R¬≤ por fold:\", scores)\n",
    "print(f\"Media R¬≤: {scores.mean():.4f}\")\n",
    "print(f\"Desviaci√≥n est√°ndar: {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6affb5",
   "metadata": {},
   "source": [
    "Observamos:\n",
    "\n",
    "Nivel de rendimiento:\n",
    "- Media R¬≤ ‚âà 0.8576 significa que, en promedio, el modelo explica alrededor del 85,8% de la variabilidad del ingreso en los datos de validaci√≥n.\n",
    "- Esto es menor al R¬≤ ‚âà 0.94 que obtuviste en el train/test split √∫nico, lo que es normal porque la validaci√≥n cruzada da una estimaci√≥n m√°s realista y menos optimista.\n",
    "\n",
    "Estabilidad del modelo:\n",
    "- Desviaci√≥n est√°ndar: 0.0127 ‚Üí muy baja, lo que indica que el rendimiento es consistente entre folds y que el modelo es estable ante cambios en el conjunto de entrenamiento.\n",
    "- Un desv√≠o bajo sugiere que el modelo no depende demasiado de un subconjunto particular de los datos.\n",
    "\n",
    "Posible sobreajuste:\n",
    "- La diferencia entre el R¬≤ alto en train/test √∫nico y el R¬≤ de CV sugiere que el modelo podr√≠a estar sobreajustando levemente.\n",
    "- Esto se puede trabajar ajustando hiperpar√°metros (max_depth, min_samples_split, etc.) o reduciendo dimensionalidad.\n",
    "\n",
    "Conclusi√≥n:\n",
    "- El modelo es fuerte y estable, pero el gap entre test √∫nico y CV sugiere que todav√≠a hay margen para mejorar generalizaci√≥n.\n",
    "- Ya est√° en un rango de rendimiento alto para este tipo de problema, as√≠ que es un buen candidato para afinar con GridSearch o RandomizedSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea92f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) XGBoost (Modelo 3)\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "\n",
    "    modelo_xgb = XGBRegressor(\n",
    "        n_estimators=800,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "        reg_alpha=0.0,\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\",     # r√°pido y estable\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    res_xgb = evaluar_modelo(modelo_xgb, X_train_proc, y_train, X_test_proc, y_test, nombre=\"XGBoost\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\n[AVISO] xgboost no est√° instalado. Ejecuta: pip install xgboost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6296ca",
   "metadata": {},
   "source": [
    "Comparaci√≥n con Random Forest (que reportaste antes)\n",
    "- Random Forest: R¬≤ = 0.9398 | MAE_pesos ‚âà 11 394 | RMSE_pesos ‚âà 33 638\n",
    "- XGBoost: R¬≤ = 0.9335 | MAE_pesos ‚âà 13 736 | RMSE_pesos ‚âà 32 255\n",
    "\n",
    "Lectura de los resultados:\n",
    "- R¬≤: Muy parecidos, aunque el RF todav√≠a tiene una leve ventaja en tu dataset.\n",
    "- Error absoluto medio (MAE): El XGBoost tiene un MAE en pesos un poco m√°s alto que el RF (‚âà +2 300 pesos), lo que indica que, en promedio, sus predicciones se desv√≠an un poco m√°s del valor real.\n",
    "- Error cuadr√°tico medio (RMSE): Aqu√≠ el XGBoost gana levemente (32 255 vs 33 638), lo que sugiere que maneja mejor los casos extremos y reduce el impacto de los outliers en el error promedio cuadr√°tico.\n",
    "\n",
    "Conclusi√≥n:\n",
    "- Ambos modelos son fuertes candidatos y est√°n muy parejos.\n",
    "- El RF parece un poco m√°s preciso en predicciones promedio (mejor MAE), mientras que XGBoost parece algo m√°s robusto con valores extremos (mejor RMSE).\n",
    "- Esto es exactamente el tipo de escenario donde hacer cross-validation en igualdad de condiciones para ambos y luego un tuning de hiperpar√°metros puede inclinar la balanza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c663723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) LightGBM (Modelo 4)\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "\n",
    "    modelo_lgb = LGBMRegressor(\n",
    "        n_estimators=1200,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=63,          # ~ 2^(max_depth) aprox (si max_depth ~ 6)\n",
    "        max_depth=-1,           # sin l√≠mite (usa num_leaves como control)\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    res_lgb = evaluar_modelo(modelo_lgb, X_train_proc, y_train, X_test_proc, y_test, nombre=\"LightGBM\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\n[AVISO] lightgbm no est√° instalado. Ejecuta: pip install lightgbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da2d60b",
   "metadata": {},
   "source": [
    "Lectura de los resultados:\n",
    "- R¬≤: RF sigue siendo el que m√°s explica la varianza. LGBM est√° apenas 0.0032 puntos por debajo de RF y mejora a XGB.\n",
    "- MAE_pesos (error promedio): RF sigue ganando en error absoluto promedio. LGBM mejora a XGB en MAE, pero todav√≠a no alcanza a RF.\n",
    "- RMSE_pesos (penaliza outliers): Aqu√≠ LGBM es el mejor de los tres, lo que indica que maneja muy bien valores extremos y reduce el impacto de grandes desviaciones.\n",
    "\n",
    "Conclusiones:\n",
    "- RF = mejor en precisi√≥n media (MAE m√°s bajo).\n",
    "- LGBM = mejor en robustez frente a valores extremos (RMSE m√°s bajo).\n",
    "- XGB = intermedio, pero queda por debajo de LGBM en casi todo.\n",
    "- Te importa predecir con menor error promedio, RF afinado es el candidato.\n",
    "- Te importa evitar grandes errores puntuales, LGBM afinado podr√≠a ser la elecci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce21757",
   "metadata": {},
   "source": [
    "Random Forest con GridSearchCV explota la memoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2655a849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Random Forest con GridSearchCV (Modelo 5)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# B√∫squeda en una grilla compacta pero informativa\n",
    "param_grid_rf = {\n",
    "    \"n_estimators\": [300, 500, 800],\n",
    "    \"max_depth\": [None, 12, 20],\n",
    "    \"min_samples_split\": [2, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "rf_base = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Usamos RMSE negativo como scoring (minimizar RMSE)\n",
    "grid_rf = GridSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_grid=param_grid_rf,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Ajuste con TRAIN ya preprocesado\n",
    "grid_rf.fit(X_train_proc, y_train)\n",
    "\n",
    "print(\"\\n=== GridSearchCV RandomForest ===\")\n",
    "print(\"Mejores hiperpar√°metros:\", grid_rf.best_params_)\n",
    "print(f\"Mejor score CV (RMSE): {-grid_rf.best_score_:,.4f}\")\n",
    "\n",
    "# Evaluaci√≥n final en TEST con el mejor estimador\n",
    "mejor_rf = grid_rf.best_estimator_\n",
    "res_rf_cv = evaluar_modelo(mejor_rf, X_train_proc, y_train, X_test_proc, y_test, nombre=\"Random Forest (GridSearchCV)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ec97a3",
   "metadata": {},
   "source": [
    "Recordamos:\n",
    "- Los √°rboles/boosting suelen rendir mejor que los lineales en este tipo de datos tabulares con alta no linealidad e interacciones.\n",
    "- Si el tiempo es un factor, pod√©s reducir la grilla del RF (menos combinaciones) o usar RandomizedSearchCV.\n",
    "- Si tu y es log_YDA, las m√©tricas ‚Äúoriginales‚Äù que imprime el bloque (deslogarizando) son las m√°s interpretables para presentaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47c989b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV XGBoost\n",
    "\n",
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "\n",
    "# Scoring de RMSE (negativo para que lo maximice)\n",
    "scoring_rmse = \"neg_root_mean_squared_error\"\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    from scipy.stats import randint, uniform, loguniform\n",
    "\n",
    "    xgb_base = XGBRegressor(\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\"   # r√°pido y estable\n",
    "    )\n",
    "\n",
    "    dist_xgb = {\n",
    "        \"n_estimators\": randint(400, 1500),\n",
    "        \"max_depth\": randint(3, 12),\n",
    "        \"learning_rate\": loguniform(1e-3, 3e-1),\n",
    "        \"subsample\": uniform(0.6, 0.4),         # [0.6, 1.0]\n",
    "        \"colsample_bytree\": uniform(0.6, 0.4),  # [0.6, 1.0]\n",
    "        \"reg_lambda\": loguniform(1e-3, 10),\n",
    "        \"reg_alpha\": loguniform(1e-4, 1)\n",
    "    }\n",
    "\n",
    "    rnd_xgb = RandomizedSearchCV(\n",
    "        estimator=xgb_base,\n",
    "        param_distributions=dist_xgb,\n",
    "        n_iter=40,              # pod√©s subir/bajar seg√∫n tiempo\n",
    "        scoring=scoring_rmse,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    rnd_xgb.fit(X_train_proc, y_train)\n",
    "    print(\"\\n=== RandomizedSearchCV XGBoost ===\")\n",
    "    print(\"Mejores hiperpar√°metros:\", rnd_xgb.best_params_)\n",
    "    print(f\"Mejor score CV (RMSE): {-rnd_xgb.best_score_:,.4f}\")\n",
    "\n",
    "    best_xgb = rnd_xgb.best_estimator_\n",
    "    res_xgb = evaluar_modelo(best_xgb, X_train_proc, y_train, X_test_proc, y_test, nombre=\"XGBoost (RandomizedSearchCV)\")\n",
    "\n",
    "except ImportError:\n",
    "    best_xgb = None\n",
    "    print(\"\\n[AVISO] xgboost no est√° instalado. Ejecuta: pip install xgboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV LightGBM\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMRegressor\n",
    "    from scipy.stats import randint, uniform, loguniform\n",
    "\n",
    "    lgb_base = LGBMRegressor(\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    dist_lgb = {\n",
    "        \"n_estimators\": randint(400, 1500),\n",
    "        \"learning_rate\": loguniform(1e-3, 3e-1),\n",
    "        \"num_leaves\": randint(31, 255),\n",
    "        \"max_depth\": [-1, 6, 8, 10, 12],          # categ√≥rico\n",
    "        \"subsample\": uniform(0.6, 0.4),\n",
    "        \"colsample_bytree\": uniform(0.6, 0.4),\n",
    "        \"reg_lambda\": loguniform(1e-3, 10),\n",
    "        \"min_child_samples\": randint(5, 100)\n",
    "    }\n",
    "\n",
    "    rnd_lgb = RandomizedSearchCV(\n",
    "        estimator=lgb_base,\n",
    "        param_distributions=dist_lgb,\n",
    "        n_iter=40,\n",
    "        scoring=scoring_rmse,\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    rnd_lgb.fit(X_train_proc, y_train)\n",
    "    print(\"\\n=== RandomizedSearchCV LightGBM ===\")\n",
    "    print(\"Mejores hiperpar√°metros:\", rnd_lgb.best_params_)\n",
    "    print(f\"Mejor score CV (RMSE): {-rnd_lgb.best_score_:,.4f}\")\n",
    "\n",
    "    best_lgb = rnd_lgb.best_estimator_\n",
    "    res_lgb = evaluar_modelo(best_lgb, X_train_proc, y_train, X_test_proc, y_test, nombre=\"LightGBM (RandomizedSearchCV)\")\n",
    "\n",
    "except ImportError:\n",
    "    best_lgb = None\n",
    "    print(\"\\n[AVISO] lightgbm no est√° instalado. Ejecuta: pip install lightgbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a705d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation Importance (sobre el mejor modelo)\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Obtener nombres de features del preprocessor ya fiteado\n",
    "try:\n",
    "    feature_names = preprocessor.get_feature_names_out(X.columns)\n",
    "except Exception:\n",
    "    # fallback: √≠ndices si no hay nombres\n",
    "    feature_names = np.array([f\"f{i}\" for i in range(X_train_proc.shape[1])])\n",
    "\n",
    "# 2) Elegir el mejor modelo entre XGB y LGB (por RMSE en test)\n",
    "candidatos = []\n",
    "if 'res_xgb' in locals():\n",
    "    candidatos.append((\"XGBoost\", best_xgb, res_xgb['rmse']))\n",
    "if 'res_lgb' in locals():\n",
    "    candidatos.append((\"LightGBM\", best_lgb, res_lgb['rmse']))\n",
    "\n",
    "if candidatos:\n",
    "    ganador_nombre, ganador_modelo, _ = sorted(candidatos, key=lambda t: t[2])[0]\n",
    "    print(f\"\\nModelo ganador para interpretaci√≥n: {ganador_nombre}\")\n",
    "else:\n",
    "    # Si no hay XGB/LGB disponibles, pod√©s elegir otro (ej: mejor RF)\n",
    "    ganador_nombre, ganador_modelo = \"Modelo_no_disponible\", None\n",
    "\n",
    "# 3) Permutation importance (si hay modelo)\n",
    "if ganador_modelo is not None:\n",
    "    # Para acelerar, usar una muestra del test (opcional)\n",
    "    np.random.seed(42)\n",
    "    idx_sample = np.random.choice(X_test_proc.shape[0], size=min(3000, X_test_proc.shape[0]), replace=False)\n",
    "    X_te_pi = X_test_proc[idx_sample]\n",
    "    y_te_pi = y_test.iloc[idx_sample] if hasattr(y_test, \"iloc\") else y_test[idx_sample]\n",
    "\n",
    "    print(\"\\nCalculando Permutation Importance (esto puede tardar un poco)...\")\n",
    "    pi = permutation_importance(\n",
    "        estimator=ganador_modelo,\n",
    "        X=X_te_pi,\n",
    "        y=y_te_pi,\n",
    "        n_repeats=5,\n",
    "        scoring=\"neg_root_mean_squared_error\",\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    importancias = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"importance_mean\": pi.importances_mean,\n",
    "        \"importance_std\": pi.importances_std\n",
    "    }).sort_values(\"importance_mean\", ascending=False)\n",
    "\n",
    "    top = importancias.head(20)\n",
    "    print(\"\\nTop 20 features por Permutation Importance:\")\n",
    "    display(top)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    plt.barh(top[\"feature\"][::-1], top[\"importance_mean\"][::-1])\n",
    "    plt.title(f\"Permutation Importance - Top 20 ({ganador_nombre})\")\n",
    "    plt.xlabel(\"Mean decrease in score (|neg RMSE|)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\n[AVISO] No hay modelo ganador para calcular permutation importance.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73771a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP para el modelo ganador\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "\n",
    "    if ganador_modelo is not None:\n",
    "        # Crear DataFrame con nombres de columnas (mejora los gr√°ficos)\n",
    "        X_te_df = pd.DataFrame(X_test_proc, columns=feature_names)\n",
    "\n",
    "        # Muestra para acelerar el c√≥mputo\n",
    "        np.random.seed(42)\n",
    "        samp = min(2000, X_te_df.shape[0])\n",
    "        X_te_sample = X_te_df.sample(samp, random_state=42)\n",
    "\n",
    "        # Explainer para modelos de √°rboles (XGB/LGB/RF)\n",
    "        explainer = shap.TreeExplainer(ganador_modelo)\n",
    "        shap_values = explainer.shap_values(X_te_sample)\n",
    "\n",
    "        # Resumen global\n",
    "        shap.summary_plot(shap_values, X_te_sample, show=True, max_display=25)\n",
    "        # (Opcional) resumen por bar:\n",
    "        shap.summary_plot(shap_values, X_te_sample, plot_type=\"bar\", show=True, max_display=25)\n",
    "\n",
    "        # Nota: los SHAP est√°n en la misma escala que el target (log_YDA).\n",
    "    else:\n",
    "        print(\"\\n[AVISO] No hay modelo ganador para calcular SHAP.\")\n",
    "\n",
    "except ImportError:\n",
    "    print(\"\\n[AVISO] shap no est√° instalado. Ejecuta: pip install shap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825c1667",
   "metadata": {},
   "source": [
    "Consideramos:\n",
    "- Nombres de features: preprocessor.get_feature_names_out(X.columns) te devuelve los nombres ya expandidos por el OneHot; as√≠ los gr√°ficos y tablas quedan legibles.\n",
    "- Tiempo de c√≥mputo: tanto RandomizedSearchCV como SHAP pueden tardar; por eso limito n_iter y tomo muestras de test para importancia/SHAP.\n",
    "- Interpretaci√≥n: SHAP y permutation importance reportan contribuciones en escala de y. Como vos us√°s log_YDA, las contribuciones est√°n en log-ingreso (lo se√±al√°s en el informe)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4928f65",
   "metadata": {},
   "source": [
    "Tabla comparativa de modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0bfa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Funci√≥n auxiliar para evaluar en log y en escala original (pesos)\n",
    "def evaluar_modelo_full(nombre, modelo, X_tr_proc, X_te_proc, y_tr, y_te):\n",
    "    # En log\n",
    "    y_pred_train = modelo.predict(X_tr_proc)\n",
    "    y_pred_test  = modelo.predict(X_te_proc)\n",
    "\n",
    "    mae_log  = mean_absolute_error(y_te, y_pred_test)\n",
    "    rmse_log = np.sqrt(mean_squared_error(y_te, y_pred_test))\n",
    "    r2_log   = r2_score(y_te, y_pred_test)\n",
    "\n",
    "    # A escala original\n",
    "    y_te_pesos    = np.exp(y_te)\n",
    "    y_pred_pesos  = np.exp(y_pred_test)\n",
    "\n",
    "    mae_pesos  = mean_absolute_error(y_te_pesos, y_pred_pesos)\n",
    "    rmse_pesos = np.sqrt(mean_squared_error(y_te_pesos, y_pred_pesos))\n",
    "    r2_pesos   = r2_score(y_te_pesos, y_pred_pesos)\n",
    "\n",
    "    return {\n",
    "        \"Modelo\": nombre,\n",
    "        \"MAE_log\": mae_log,\n",
    "        \"RMSE_log\": rmse_log,\n",
    "        \"R2_log\": r2_log,\n",
    "        \"MAE_pesos\": mae_pesos,\n",
    "        \"RMSE_pesos\": rmse_pesos,\n",
    "        \"R2_pesos\": r2_pesos\n",
    "    }\n",
    "\n",
    "# Lista de modelos ya entrenados y sus nombres\n",
    "modelos_resultados = []\n",
    "\n",
    "# Modelo 1: Regresi√≥n lineal\n",
    "modelos_resultados.append(evaluar_modelo_full(\"LinearRegression\", modelo_lr, X_train_proc, X_test_proc, y_train, y_test))\n",
    "\n",
    "# Modelo 2: Random Forest b√°sico\n",
    "modelos_resultados.append(evaluar_modelo_full(\"RandomForest\", modelo_rf, X_train_proc, X_test_proc, y_train, y_test))\n",
    "\n",
    "# Modelo 3: XGBoost optimizado\n",
    "modelos_resultados.append(evaluar_modelo_full(\"XGBoost\", best_xgb, X_train_proc, X_test_proc, y_train, y_test))\n",
    "\n",
    "# Modelo 4: LightGBM optimizado\n",
    "modelos_resultados.append(evaluar_modelo_full(\"LightGBM\", best_lgb, X_train_proc, X_test_proc, y_train, y_test))\n",
    "\n",
    "# Modelo 5: Random Forest con GridSearchCV\n",
    "modelos_resultados.append(evaluar_modelo_full(\"RandomForest (tuned)\", best_rf, X_train_proc, X_test_proc, y_train, y_test))\n",
    "\n",
    "# Crear tabla resumen\n",
    "tabla_resultados = pd.DataFrame(modelos_resultados)\n",
    "\n",
    "# Ordenar por RMSE_log ascendente (mejor modelo arriba)\n",
    "tabla_resultados = tabla_resultados.sort_values(by=\"RMSE_log\")\n",
    "\n",
    "# Mostrar con 3 decimales\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:,.3f}\")\n",
    "display(tabla_resultados)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681cd4d6",
   "metadata": {},
   "source": [
    "Probamos con RandomizedSearchCV para RF, XGBoost y LightGBM con float32 y cv=3 y comparativo de modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906991d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to interrupt the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.11.5)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# =====================\n",
    "# 1) Preprocesar con float32 para reducir memoria\n",
    "# =====================\n",
    "X_train_proc = preprocessor.fit_transform(X_train).astype(np.float32)\n",
    "X_test_proc  = preprocessor.transform(X_test).astype(np.float32)\n",
    "\n",
    "# =====================\n",
    "# 2) Funci√≥n para evaluaci√≥n r√°pida\n",
    "# =====================\n",
    "def evaluar_modelo_simple(modelo, X_train, y_train, X_test, y_test, nombre=\"Modelo\"):\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_test)\n",
    "\n",
    "    mae_log = mean_absolute_error(y_test, y_pred)\n",
    "    rmse_log = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Revertimos log a pesos\n",
    "    y_test_original = np.exp(y_test)\n",
    "    y_pred_original = np.exp(y_pred)\n",
    "    mae_pesos = mean_absolute_error(y_test_original, y_pred_original)\n",
    "    rmse_pesos = np.sqrt(mean_squared_error(y_test_original, y_pred_original))\n",
    "\n",
    "    return {\n",
    "        \"Modelo\": nombre,\n",
    "        \"R2\": r2,\n",
    "        \"MAE_log\": mae_log,\n",
    "        \"RMSE_log\": rmse_log,\n",
    "        \"MAE_pesos\": mae_pesos,\n",
    "        \"RMSE_pesos\": rmse_pesos\n",
    "    }\n",
    "\n",
    "# =====================\n",
    "# 3) RandomizedSearchCV para cada modelo\n",
    "# =====================\n",
    "\n",
    "# --- Random Forest ---\n",
    "param_rf = {\n",
    "    \"n_estimators\": [200, 400, 600],\n",
    "    \"max_depth\": [10, 15, 20, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "search_rf = RandomizedSearchCV(rf, param_rf, n_iter=10, cv=3, scoring='r2',\n",
    "                               verbose=2, random_state=42, n_jobs=-1)\n",
    "t0 = time.time()\n",
    "search_rf.fit(X_train_proc, y_train)\n",
    "print(\"Random Forest mejores params:\", search_rf.best_params_, \"Tiempo:\", round(time.time()-t0, 2), \"s\")\n",
    "\n",
    "# --- XGBoost ---\n",
    "param_xgb = {\n",
    "    \"n_estimators\": [200, 400, 600],\n",
    "    \"max_depth\": [3, 5, 7, 10],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"subsample\": [0.7, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 1.0]\n",
    "}\n",
    "xgb = XGBRegressor(objective='reg:squarederror', random_state=42, n_jobs=-1)\n",
    "search_xgb = RandomizedSearchCV(xgb, param_xgb, n_iter=10, cv=3, scoring='r2',\n",
    "                                verbose=2, random_state=42, n_jobs=-1)\n",
    "t0 = time.time()\n",
    "search_xgb.fit(X_train_proc, y_train)\n",
    "print(\"XGBoost mejores params:\", search_xgb.best_params_, \"Tiempo:\", round(time.time()-t0, 2), \"s\")\n",
    "\n",
    "# --- LightGBM ---\n",
    "param_lgb = {\n",
    "    \"n_estimators\": [200, 400, 600],\n",
    "    \"max_depth\": [-1, 10, 15, 20],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"subsample\": [0.7, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 1.0]\n",
    "}\n",
    "lgb = LGBMRegressor(objective='regression', random_state=42, n_jobs=-1)\n",
    "search_lgb = RandomizedSearchCV(lgb, param_lgb, n_iter=10, cv=3, scoring='r2',\n",
    "                                verbose=2, random_state=42, n_jobs=-1)\n",
    "t0 = time.time()\n",
    "search_lgb.fit(X_train_proc, y_train)\n",
    "print(\"LightGBM mejores params:\", search_lgb.best_params_, \"Tiempo:\", round(time.time()-t0, 2), \"s\")\n",
    "\n",
    "# =====================\n",
    "# 4) Evaluar y comparar\n",
    "# =====================\n",
    "resultados = []\n",
    "resultados.append(evaluar_modelo_simple(search_rf.best_estimator_, X_train_proc, y_train, X_test_proc, y_test, \"Random Forest\"))\n",
    "resultados.append(evaluar_modelo_simple(search_xgb.best_estimator_, X_train_proc, y_train, X_test_proc, y_test, \"XGBoost\"))\n",
    "resultados.append(evaluar_modelo_simple(search_lgb.best_estimator_, X_train_proc, y_train, X_test_proc, y_test, \"LightGBM\"))\n",
    "\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "print(\"\\nüìä Comparativa de modelos:\")\n",
    "print(df_resultados)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
