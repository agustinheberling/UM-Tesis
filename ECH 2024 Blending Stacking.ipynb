{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d08fe1d-250f-45ed-bc5e-bcba8bbb022a",
   "metadata": {},
   "source": [
    "## Analisis de la Encuesta Continua de Hogares 2024\n",
    "\n",
    "- Dataset: https://www4.ine.gub.uy/Anda5/index.php/catalog/767/get-microdata\n",
    "- Diccionario: https://www4.ine.gub.uy/Anda5/index.php/catalog/767/data-dictionary/F4?file_name=ECH_implantacion_2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2427de3c",
   "metadata": {},
   "source": [
    "## Comienza el notebook de los modelos de combinacion lineal blending y stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbb4e37",
   "metadata": {},
   "source": [
    "## Carga del dataset depurado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "617142b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTAMOS LIBRERIAS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ea69066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes cargados:\n",
      "X: (55923, 2846)\n",
      "y: (55923,)\n"
     ]
    }
   ],
   "source": [
    "# Cargar datasets limpios\n",
    "X = pd.read_csv(r'D:\\ut603933\\Tesis-MCD\\data_processed\\X_clean.csv')\n",
    "y = pd.read_csv(r'D:\\ut603933\\Tesis-MCD\\data_processed\\y_clean.csv').squeeze()  # .squeeze() para que sea Serie y no DataFrame\n",
    "\n",
    "print(\"Shapes cargados:\")\n",
    "print(\"X:\", X.shape)\n",
    "print(\"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaa6419",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82b709e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes tras split:\n",
      "X_train: (44738, 2846)\n",
      "X_test: (11185, 2846)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Shapes tras split:\")\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93353bf",
   "metadata": {},
   "source": [
    "Conservamos la funcion para evaluar los diferentes modelos con metricas en train y en test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f664f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "def evaluar_modelo(modelo, X_train, y_train, X_test, y_test, nombre=\"modelo\"):\n",
    "    print(\"ðŸŸ¢ Entrenando modelo...\")\n",
    "    modelo.fit(X_train, y_train)\n",
    "\n",
    "    print(\"ðŸŸ¢ Generando predicciones...\")\n",
    "    y_pred_train = modelo.predict(X_train)\n",
    "    y_pred_test = modelo.predict(X_test)\n",
    "\n",
    "    print(\"ðŸ” Verificando predicciones...\")\n",
    "    print(f\"MÃ¡ximo y_pred_test: {np.max(y_pred_test)}\")\n",
    "    print(f\"MÃ­nimo y_pred_test: {np.min(y_pred_test)}\")\n",
    "\n",
    "    # ========= MÃ‰TRICAS EN ESCALA LOG =========\n",
    "    print(\"ðŸ“Š Calculando mÃ©tricas en escala log...\")\n",
    "\n",
    "    # --- Train ---\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "\n",
    "    # --- Test ---\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "    rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "\n",
    "    # ========= MÃ‰TRICAS EN ESCALA ORIGINAL (PESOS) =========\n",
    "    # Clip para evitar overflow al hacer exp()\n",
    "    y_train_clip = np.clip(y_train, 0, 30)\n",
    "    y_pred_train_clip = np.clip(y_pred_train, 0, 30)\n",
    "    y_test_clip = np.clip(y_test, 0, 30)\n",
    "    y_pred_test_clip = np.clip(y_pred_test, 0, 30)\n",
    "\n",
    "    y_train_original = np.exp(y_train_clip)\n",
    "    y_pred_train_original = np.exp(y_pred_train_clip)\n",
    "    y_test_original = np.exp(y_test_clip)\n",
    "    y_pred_test_original = np.exp(y_pred_test_clip)\n",
    "\n",
    "    print(f\"MÃ¡ximo y_pred_original (clipped test): {np.max(y_pred_test_original)}\")\n",
    "\n",
    "    # --- Train pesos ---\n",
    "    mae_train_pesos = mean_absolute_error(y_train_original, y_pred_train_original)\n",
    "    rmse_train_pesos = np.sqrt(mean_squared_error(y_train_original, y_pred_train_original))\n",
    "\n",
    "    # --- Test pesos ---\n",
    "    mae_test_pesos = mean_absolute_error(y_test_original, y_pred_test_original)\n",
    "    rmse_test_pesos = np.sqrt(mean_squared_error(y_test_original, y_pred_test_original))\n",
    "\n",
    "    # ========= OUTPUT =========\n",
    "    print(\"âœ… MÃ©tricas finales:\")\n",
    "    print(f\"Modelo: {nombre}\")\n",
    "\n",
    "    print(f\"[Train] R2: {r2_train:.4f} | MAE_log: {mae_train:.2f} | RMSE_log: {rmse_train:.2f}\")\n",
    "    print(f\"[Train] MAE_pesos: {mae_train_pesos:.2f} | RMSE_pesos: {rmse_train_pesos:.2f}\")\n",
    "\n",
    "    print(f\"[Test]  R2: {r2_test:.4f} | MAE_log: {mae_test:.2f} | RMSE_log: {rmse_test:.2f}\")\n",
    "    print(f\"[Test]  MAE_pesos: {mae_test_pesos:.2f} | RMSE_pesos: {rmse_test_pesos:.2f}\")\n",
    "\n",
    "    return {\n",
    "        \"modelo\": nombre,\n",
    "        \"R2_train\": r2_train,\n",
    "        \"MAE_log_train\": mae_train,\n",
    "        \"RMSE_log_train\": rmse_train,\n",
    "        \"MAE_pesos_train\": mae_train_pesos,\n",
    "        \"RMSE_pesos_train\": rmse_train_pesos,\n",
    "        \"R2_test\": r2_test,\n",
    "        \"MAE_log_test\": mae_test,\n",
    "        \"RMSE_log_test\": rmse_test,\n",
    "        \"MAE_pesos_test\": mae_test_pesos,\n",
    "        \"RMSE_pesos_test\": rmse_test_pesos\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1533ad4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12026167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7737c9cb",
   "metadata": {},
   "source": [
    "## Mejores modelos de Random Forest y xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c3b3961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "âœ… Mejor Random Forest: {'n_estimators': 600, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': None}\n"
     ]
    }
   ],
   "source": [
    "# Definir hiperparÃ¡metros y RandomizedSearchCV para Random Forest\n",
    "param_rf = {\n",
    "    \"n_estimators\": [200, 400, 600],\n",
    "    \"max_depth\": [10, 15, 20, None],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "search_rf = RandomizedSearchCV(rf, param_rf, n_iter=10, cv=3, scoring=\"r2\",\n",
    "                               random_state=42, n_jobs=-1, verbose=2)\n",
    "search_rf.fit(X_train, y_train)\n",
    "best_rf = search_rf.best_estimator_\n",
    "print(\"âœ… Mejor Random Forest:\", search_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e596e8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "âœ… Mejor XGBoost: {'subsample': 0.8, 'n_estimators': 600, 'max_depth': 5, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# Definir hiperparÃ¡metros y RandomizedSearchCV para xgboost\n",
    "param_xgb = {\n",
    "    \"n_estimators\": [200, 400, 600],\n",
    "    \"max_depth\": [3, 5, 7, 10],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"subsample\": [0.7, 0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 1.0]\n",
    "}\n",
    "xgb = XGBRegressor(objective=\"reg:squarederror\", random_state=42, n_jobs=-1)\n",
    "search_xgb = RandomizedSearchCV(xgb, param_xgb, n_iter=10, cv=3, scoring=\"r2\",\n",
    "                                random_state=42, n_jobs=-1, verbose=2)\n",
    "search_xgb.fit(X_train, y_train)\n",
    "best_xgb = search_xgb.best_estimator_\n",
    "print(\"âœ… Mejor XGBoost:\", search_xgb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99cc11af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Entrenando modelo...\n",
      "ðŸŸ¢ Generando predicciones...\n",
      "ðŸ” Verificando predicciones...\n",
      "MÃ¡ximo y_pred_test: 13.951850895584109\n",
      "MÃ­nimo y_pred_test: 2.8854443878509044\n",
      "ðŸ“Š Calculando mÃ©tricas en escala log...\n",
      "MÃ¡ximo y_pred_original (clipped test): 1146071.8786031357\n",
      "âœ… MÃ©tricas finales:\n",
      "Modelo: Random Forest Optimizado\n",
      "[Train] R2: 0.9882 | MAE_log: 0.04 | RMSE_log: 0.07\n",
      "[Train] MAE_pesos: 5031.06 | RMSE_pesos: 17581.47\n",
      "[Test]  R2: 0.9367 | MAE_log: 0.10 | RMSE_log: 0.17\n",
      "[Test]  MAE_pesos: 11813.12 | RMSE_pesos: 34596.79\n",
      "ðŸŸ¢ Entrenando modelo...\n",
      "ðŸŸ¢ Generando predicciones...\n",
      "ðŸ” Verificando predicciones...\n",
      "MÃ¡ximo y_pred_test: 14.895726203918457\n",
      "MÃ­nimo y_pred_test: 2.91107439994812\n",
      "ðŸ“Š Calculando mÃ©tricas en escala log...\n",
      "MÃ¡ximo y_pred_original (clipped test): 2945314.75\n",
      "âœ… MÃ©tricas finales:\n",
      "Modelo: XGBoost Optimizado\n",
      "[Train] R2: 0.9511 | MAE_log: 0.11 | RMSE_log: 0.15\n",
      "[Train] MAE_pesos: 12755.46 | RMSE_pesos: 22632.87\n",
      "[Test]  R2: 0.9154 | MAE_log: 0.14 | RMSE_log: 0.19\n",
      "[Test]  MAE_pesos: 16259.64 | RMSE_pesos: 36975.82\n"
     ]
    }
   ],
   "source": [
    "# Evaluar modelos optimizados\n",
    "res_rf = evaluar_modelo(best_rf, X_train, y_train, X_test, y_test, nombre=\"Random Forest Optimizado\")\n",
    "res_xgb = evaluar_modelo(best_xgb, X_train, y_train, X_test, y_test, nombre=\"XGBoost Optimizado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4645004e",
   "metadata": {},
   "source": [
    "## Modelo Blending (promedio simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad91b375",
   "metadata": {},
   "source": [
    "Blending no es un modelo en sÃ­ mismo de scikit-learn sino que es el promedio de predicciones de dos modelos ya entrenados. Por eso no se puede pasar directamente la funcion a evaluar_modelo. Por lo tanto, envolvemos ese promedio dentro de un modelo personalizado compatible con scikit-learn, creando una clase que implemente .fit() y .predict(). AsÃ­ podemos evaluar el blending exactamente igual que los demÃ¡s con tu funciÃ³n evaluar_modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0f4e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# Clase para Blending\n",
    "class BlendingRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, modelos):\n",
    "        self.modelos = modelos\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Entrenamos cada modelo\n",
    "        for modelo in self.modelos:\n",
    "            modelo.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Promediamos las predicciones\n",
    "        preds = np.column_stack([modelo.predict(X) for modelo in self.modelos])\n",
    "        return np.mean(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c27d4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŸ¢ Evaluando Blending (RF + XGB)...\n",
      "ðŸŸ¢ Entrenando modelo...\n",
      "ðŸŸ¢ Generando predicciones...\n",
      "ðŸ” Verificando predicciones...\n",
      "MÃ¡ximo y_pred_test: 14.423788549751283\n",
      "MÃ­nimo y_pred_test: 2.898259393899512\n",
      "ðŸ“Š Calculando mÃ©tricas en escala log...\n",
      "MÃ¡ximo y_pred_original (clipped test): 1837264.8879587846\n",
      "âœ… MÃ©tricas finales:\n",
      "Modelo: Blending (RF+XGB)\n",
      "[Train] R2: 0.9768 | MAE_log: 0.07 | RMSE_log: 0.10\n",
      "[Train] MAE_pesos: 8612.80 | RMSE_pesos: 18536.90\n",
      "[Test]  R2: 0.9340 | MAE_log: 0.11 | RMSE_log: 0.17\n",
      "[Test]  MAE_pesos: 13433.24 | RMSE_pesos: 33578.28\n"
     ]
    }
   ],
   "source": [
    "# Blending (promedio simple)\n",
    "print(\"\\nðŸŸ¢ Evaluando Blending (RF + XGB)...\")\n",
    "blend_model = BlendingRegressor(modelos=[best_rf, best_xgb])\n",
    "res_blend = evaluar_modelo(blend_model, X_train, y_train, X_test, y_test, nombre=\"Blending (RF+XGB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2241c56",
   "metadata": {},
   "source": [
    "## Modelo Stacking (RF + XGB -> LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4e78f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŸ¢ Evaluando Stacking (RF + XGB -> LR)...\n",
      "ðŸŸ¢ Entrenando modelo...\n",
      "ðŸŸ¢ Generando predicciones...\n",
      "ðŸ” Verificando predicciones...\n",
      "MÃ¡ximo y_pred_test: 14.312100191016981\n",
      "MÃ­nimo y_pred_test: 2.557745822606508\n",
      "ðŸ“Š Calculando mÃ©tricas en escala log...\n",
      "MÃ¡ximo y_pred_original (clipped test): 1643108.1030942453\n",
      "âœ… MÃ©tricas finales:\n",
      "Modelo: Stacking RF+XGB -> LR\n",
      "[Train] R2: 0.9849 | MAE_log: 0.06 | RMSE_log: 0.08\n",
      "[Train] MAE_pesos: 6704.26 | RMSE_pesos: 15509.65\n",
      "[Test]  R2: 0.9377 | MAE_log: 0.10 | RMSE_log: 0.17\n",
      "[Test]  MAE_pesos: 12377.27 | RMSE_pesos: 32303.25\n"
     ]
    }
   ],
   "source": [
    "# Stacking (RF + XGB -> LinearRegression)\n",
    "print(\"\\nðŸŸ¢ Evaluando Stacking (RF + XGB -> LR)...\")\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=[(\"RF\", best_rf), (\"XGB\", best_xgb)],\n",
    "    final_estimator=LinearRegression(),\n",
    "    cv=3, n_jobs=-1\n",
    ")\n",
    "\n",
    "res_stacking = evaluar_modelo(stacking_model, X_train, y_train, X_test, y_test, nombre=\"Stacking RF+XGB -> LR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a77159d",
   "metadata": {},
   "source": [
    "## Comparacion de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e877f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Comparativa de modelos (OptimizaciÃ³n + Ensemblings):\n",
      "                     modelo  R2_train  MAE_log_train  RMSE_log_train  \\\n",
      "0  Random Forest Optimizado  0.988210       0.039193        0.074590   \n",
      "1        XGBoost Optimizado  0.951129       0.114570        0.151859   \n",
      "2         Blending (RF+XGB)  0.976805       0.074412        0.104619   \n",
      "3     Stacking RF+XGB -> LR  0.984924       0.057073        0.084344   \n",
      "\n",
      "   MAE_pesos_train  RMSE_pesos_train   R2_test  MAE_log_test  RMSE_log_test  \\\n",
      "0      5031.060495      17581.469757  0.936717      0.095320       0.168195   \n",
      "1     12755.464407      22632.868165  0.915410      0.138801       0.194459   \n",
      "2      8612.797405      18536.900473  0.934009      0.112234       0.171756   \n",
      "3      6704.256165      15509.650507  0.937744      0.103123       0.166825   \n",
      "\n",
      "   MAE_pesos_test  RMSE_pesos_test  \n",
      "0    11813.118758     34596.788107  \n",
      "1    16259.640911     36975.824651  \n",
      "2    13433.237170     33578.275313  \n",
      "3    12377.265874     32303.251363  \n"
     ]
    }
   ],
   "source": [
    "# Comparativa final\n",
    "df_resultados = pd.DataFrame([res_rf, res_xgb, res_blend, res_stacking])\n",
    "print(\"\\nðŸ“Š Comparativa de modelos (OptimizaciÃ³n + Ensemblings):\")\n",
    "print(df_resultados)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
